[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Autoregresivos",
    "section": "",
    "text": "Presentación\nProyecto de Estadística de Modelos Autoregresivos.\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html",
    "href": "Caps/01-Introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Modelos Fundamentales en Series de Tiempo\nEl análisis de series de tiempo es una disciplina estadística clave para estudiar fenómenos dinámicos que evolucionan a lo largo del tiempo. Desde variables económicas como la inflación y la producción industrial, hasta métricas ambientales como las temperaturas globales o el tráfico en redes informáticas, las series de tiempo permiten desentrañar patrones temporales —como tendencias, estacionalidad o ciclos— que son críticos para la toma de decisiones basada en datos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#modelos-fundamentales-en-series-de-tiempo",
    "href": "Caps/01-Introduccion.html#modelos-fundamentales-en-series-de-tiempo",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1.1 Modelos Autorregresivos (AR)\nLos modelos AR(p) suponen que el valor actual de una serie (\\(x_t\\)) depende linealmente de sus \\(p\\) valores pasados:\n\\[\nx_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\ldots + \\alpha_p x_{t-p} + \\varepsilon_t,\n\\]\ndonde \\(\\varepsilon_t\\) es ruido blanco. Son ideales para capturar dependencias secuenciales en datos estacionarios.\n\n\n1.1.2 Modelos de Media Móvil (MA)\nLos modelos MA(q) relacionan el valor actual con errores pasados:\n\\[\nx_t = \\varepsilon_t + \\beta_1 \\varepsilon_{t-1} + \\beta_2 \\varepsilon_{t-2} + \\ldots + \\beta_q \\varepsilon_{t-q}.\n\\]\nEstos modelos son útiles para describir impactos temporales de eventos aleatorios.\n\n\n1.1.3 Modelos ARMA: Combinando lo Mejor de AR y MA\nLos modelos ARMA(p, q) integran componentes autorregresivos y de media móvil:\n\\[\nx_t = \\sum_{i=1}^p \\alpha_i x_{t-i} + \\varepsilon_t + \\sum_{j=1}^q \\beta_j \\varepsilon_{t-j}.\n\\]\nSon eficaces para series estacionarias con patrones mixtos de dependencia.\n\n\n1.1.4 Modelos ARIMA: Extendiendo ARMA a Series No Estacionarias\nLos modelos ARIMA(p, d, q) aplican diferenciación (orden \\(d\\)) para convertir series no estacionarias en estacionarias antes de usar ARMA:\n\\[\n(1 - B)^d x_t = \\varepsilon_t + \\sum_{i=1}^p \\alpha_i (1 - B)^d x_{t-i} + \\sum_{j=1}^q \\beta_j \\varepsilon_{t-j},\n\\]\ndonde \\(B\\) es el operador de desplazamiento hacia atrás. Son esenciales para manejar tendencias o varianzas variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#objetivo-y-estructura-del-documento",
    "href": "Caps/01-Introduccion.html#objetivo-y-estructura-del-documento",
    "title": "1  Introducción",
    "section": "1.2 Objetivo y Estructura del Documento",
    "text": "1.2 Objetivo y Estructura del Documento\nEste documento profundiza en estos modelos, combinando el rigor teórico de Montgomery, Jennings y Kulahci (2008) con las implementaciones prácticas de Cowpertwait y Metcalfe (2009). La exposición se organiza en cuatro pilares:\n\nFundamentos Teóricos\n\nEstacionariedad, pruebas de Dickey-Fuller y herramientas diagnósticas (ACF, PACF).\n\n\nFormulación Matemática\n\nEcuaciones y propiedades de modelos AR, MA, ARMA y ARIMA.\n\n\nEstimación y Validación\n\nMétodos de máxima verosimilitud, criterios de información (AIC/BIC) y diagnóstico de residuos.\n\n\nAplicaciones Prácticas\n\nCasos de estudio en finanzas (pronóstico de precios), logística (demanda de inventario) y climatología.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#relevancia-en-la-era-de-los-datos",
    "href": "Caps/01-Introduccion.html#relevancia-en-la-era-de-los-datos",
    "title": "1  Introducción",
    "section": "1.3 Relevancia en la Era de los Datos",
    "text": "1.3 Relevancia en la Era de los Datos\nLos modelos de series de tiempo no solo son herramientas académicas: son pilares en predicción de mercados financieros, gestión de recursos energéticos y monitoreo de cambio climático. Su capacidad para descomponer complejidad temporal en componentes interpretables los hace indispensables en un mundo impulsado por datos dinámicos.\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html",
    "href": "Caps/02-Series de Tiempo.html",
    "title": "2  Series de Tiempo",
    "section": "",
    "text": "2.1 ¿Qué es una Serie de Tiempo?\nUna serie de tiempo es una secuencia de observaciones numéricas registradas en intervalos regulares a lo largo del tiempo. Entre los ejemplos más comunes se encuentran las temperaturas mensuales en una ciudad, los precios diarios de acciones en la bolsa o las ventas trimestrales de un producto. Su característica distintiva es la dependencia temporal, es decir, que el valor actual suele estar influenciado por observaciones pasadas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#fundamento-estructura-y-componentes",
    "href": "Caps/02-Series de Tiempo.html#fundamento-estructura-y-componentes",
    "title": "2  Series de Tiempo",
    "section": "2.2 Fundamento, estructura y componentes",
    "text": "2.2 Fundamento, estructura y componentes\n\n2.2.1 Definición\nFormalmente, una serie de tiempo es una colección ordenada de datos cuantitativos observados en momentos sucesivos. Esta naturaleza cronológica implica que las observaciones no son independientes, sino que tienden a mostrar correlaciones a lo largo del tiempo.\n\n\n2.2.2 Elementos Clave\nLas series de tiempo poseen varios aspectos estructurales fundamentales:\n\nOrden Cronológico: Las observaciones están dispuestas secuencialmente en el tiempo, lo que permite analizar cómo evolucionan los fenómenos observados.\nFrecuencia de Medición: Define la resolución temporal del análisis. Puede variar desde intervalos muy cortos (como milisegundos en transacciones bursátiles) hasta intervalos largos (como trimestres en reportes económicos).\nComponentes Sistemáticos: Las series de tiempo suelen mostrar patrones recurrentes además de fluctuaciones aleatorias. Estos componentes incluyen:\n\nTendencia: Cambios a largo plazo, ya sea ascendentes o descendentes.\nEstacionalidad: Fluctuaciones regulares en intervalos fijos, como los aumentos en ventas durante diciembre.\nCiclicidad: Oscilaciones de largo plazo no necesariamente periódicas, asociadas a factores económicos.\nRuido: Variabilidad no sistemática que no puede explicarse por los componentes anteriores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#componentes-fundamentales",
    "href": "Caps/02-Series de Tiempo.html#componentes-fundamentales",
    "title": "2  Series de Tiempo",
    "section": "2.3 Componentes fundamentales",
    "text": "2.3 Componentes fundamentales\nLa descomposición estructural permite analizar series de tiempo dinámicas. Los cuatro componentes esenciales son: tendencia, estacionalidad, ciclo e irregularidad.\n\n2.3.1 Tendencia\nRefleja un crecimiento o decrecimiento sostenido en el tiempo. Por ejemplo, el número de pasajeros aéreos muestra un incremento constante a lo largo de 12 años, con una pendiente del 12% anual, asociada al auge del turismo y los negocios internacionales tras la Segunda Guerra Mundial.\n\n\nCódigo\n# -------------------------------\n# 1. Tendencia (Ejemplo: Pasajeros aéreos)\n# -------------------------------\ndata(\"AirPassengers\")\ntrend_data &lt;- data.frame(\n  fecha = time(AirPassengers) %&gt;% as.numeric(),\n  pasajeros = as.numeric(AirPassengers)\n)\n\nggplot(trend_data, aes(x = fecha, y = pasajeros)) +\n  geom_line(color = \"#F48FB1\", linewidth = 1) +\n  geom_smooth(method = \"lm\", color = \"#CE93D8\", se = FALSE) +\n  labs(title = \"Tendencia: Pasajeros Aéreos Internacionales (1949-1960)\",\n       x = \"Año\",\n       y = \"Número de Pasajeros (miles)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nEsta gráfica representa el número de pasajeron mensual en vuelos internacionales (1949-1960). Se observa un crecimiento sostenido debido a la expansión de la aviaci;ón comercial.\n\n\n2.3.2 Estacionalidad\nLa estacionalidad se refiere a fluctuaciones periódicas y predecibles en los datos, que se repiten en intervalos regulares (diarios, mensuales, anuales). Surge de factores recurrentes como festividades, condiciones climáticas o ciclos comerciales.\n\n\nCódigo\n# -------------------------------\n# 2. Estacionalidad \n# -------------------------------\ndata(\"AirPassengers\")\ndf_air &lt;- data.frame(\n  año = floor(time(AirPassengers)),  # Extraer año\n  mes = cycle(AirPassengers),        # Extraer mes (1-12)\n  pasajeros = as.numeric(AirPassengers)\n)\n\nggplot(df_air, aes(x = año + (mes-1)/12, y = pasajeros)) +\n  geom_line(color = \"#F48FB1\", linewidth = 0.8) +\n  geom_point(color = \"#CE93D8\", size = 1.5) +\n  labs(\n    title = \"Estacionalidad en Pasajeros Aéreos\",\n    subtitle = \"Picos recurrentes en verano (meses 6-8) y diciembre\",\n    x = \"Año\",\n    y = \"Pasajeros (miles)\"\n  ) +\n  scale_x_continuous(breaks = 1949:1960) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPodemos notar lo siguiente: 1. Picos Recurrentes:\n- Verano (Junio-Agosto):\nAumento del 35-40% en pasajeros respecto al promedio anual, impulsado por vacaciones escolares y turismo estival.\n- Diciembre:\nSegundo pico del 25-30% por viajes navideños y de fin de año.\n\nValles Pronunciados:\n\nEnero-Febrero:\nCaída del 20-25% por reducción de viajes post-festivos y clima invernal en el hemisferio norte.\n\n\n\n\n2.3.3 Ciclos\nRepresentan fluctuaciones prolongadas, pero no periódicas. Un ciclo económico puede durar entre 7 y 10 años, como la expansión entre 1991–2001 o la recesión de 2008. Sus picos y valles se asocian con innovaciones tecnológicas o crisis financieras, respectivamente.\n\n\nCódigo\n# -------------------------------\n# 3. Ciclo\n# -------------------------------\n# Cargar datos y definir recesiones (fuente: NBER)\nrecesiones &lt;- data.frame(\n  inicio = as.Date(c(\"1973-11-01\", \"1981-07-01\", \"1990-07-01\", \"2001-03-01\", \"2007-12-01\")),\n  fin = as.Date(c(\"1975-03-01\", \"1982-11-01\", \"1991-03-01\", \"2001-11-01\", \"2009-06-01\"))\n)\n\n# Gráfico con ciclos económicos\nggplot(economics) +\n  geom_line(aes(x = date, y = unemploy/1000), color = \"#1f77b4\", linewidth = 0.8) +\n  geom_rect(\n    data = recesiones,\n    aes(xmin = inicio, xmax = fin, ymin = 0, ymax = Inf),\n    fill = \"#d62728\", alpha = 0.15\n  ) +\n  labs(\n    title = \"Ciclos Económicos en el Desempleo de EE.UU.\",\n    subtitle = \"Áreas rojas: recesiones oficiales (NBER) | Datos: 1967-2015\",\n    x = \"Año\",\n    y = \"Desempleados (millones)\"\n  ) +\n  scale_x_date(date_breaks = \"10 years\", date_labels = \"%Y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSe puede apreciar fluctiaciones no periodicas vinculadas a eventos macroeconomicos. Estas se deben a la crisis petrolera (1973-75), una politica de antiinflación (Volcker Schock, 1981-82) y a una crisis financiera (2007-09).\n\n\n2.3.4 Irregularidad\nIncluye eventos impredecibles. Por ejemplo, un pico repentino en Q4/2023 por la aprobación de un ETF de Bitcoin, o una caída en Q1/2024 causada por un ciberataque. La desviación estándar del 45% refleja alta volatilidad.\n\n\nCódigo\n# -------------------------------\n# 4. Irregularidad (Ventas diarias)\n# -------------------------------\nset.seed(2024)\ndf_irregular &lt;- data.frame(\n  fecha = seq.Date(as.Date(\"2024-01-01\"), by = \"day\", length.out = 180),\n  ventas = 500 + cumsum(rnorm(180, mean = 0, sd = 3))  # Ruido acumulativo\n) \n\n# Añadir eventos atípicos\ndf_irregular$ventas[50:55] &lt;- df_irregular$ventas[50:55] + 80   # Promoción flash\ndf_irregular$ventas[120:125] &lt;- df_irregular$ventas[120:125] - 60  # Fallo logístico\n\nggplot(df_irregular, aes(x = fecha, y = ventas)) +\n  geom_line(color = \"#9467bd\", linewidth = 0.7) +\n  geom_point(data = df_irregular[c(50:55, 120:125), ], \n             color = \"#d62728\", size = 2) +\n  labs(\n    title = \"Irregularidad en Ventas Diarias\",\n    subtitle = \"Puntos rojos: eventos atípicos no explicados por patrones estructurales\",\n    x = \"Fecha\",\n    y = \"Ventas (unidades)\"\n  ) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAquí se notan fluctuaciones no sistemáticas que pueden ser causadas por eventos imprevistos, errores de medición o solo comportamientos aleatorios del mercado.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#esperanza-de-una-serie-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#esperanza-de-una-serie-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.4 Esperanza de una serie de Tiempo",
    "text": "2.4 Esperanza de una serie de Tiempo\nLa esperanza \\(E(Y_t)\\) de una serie \\(\\{Y_t\\}\\) representa su valor promedio teórico en el tiempo \\(t\\). Este concepto central se estudia con el supuesto de que el error \\(\\varepsilon_t\\sim N(0,\\sigma^2)\\) es ruido blanco gaussiano.\n\n2.4.1 Casos de Estudio\n\nMedia Constante con Ruido Blanco\nModelo:\n\\[\nY_t = \\mu + \\varepsilon_t\n\\]\nCálculo de la Esperanza:\n\\[\nE(Y_t) = E(\\mu + \\varepsilon_t) = \\mu + E(\\varepsilon_t) = \\mu + 0 = \\mu\n\\]\nInterpretación:\n- La serie fluctúa alrededor de una media constante \\(\\mu\\).\n- Ejemplo: Temperatura promedio mensual en una región estable.\n\n\nTendencia Lineal con Ruido Blanco\nModelo:\n\\[\nY_t = \\beta t + \\varepsilon_t\n\\]\nCálculo de la Esperanza:\n\\[\nE(Y_t) = E(\\beta t + \\varepsilon_t) = \\beta t + E(\\varepsilon_t) = \\beta t\n\\]\nInterpretación:\n- La media de la serie crece/decrece linealmente con el tiempo.\n- Ejemplo: Ventas anuales de un producto en expansión.\n\n\n\n2.4.2 Propiedades Clave\n\nLinealidad:\nSi \\(Y_t = a X_t + b\\), entonces \\(E(Y_t) = a E(X_t) + b\\).\nIndependencia del Ruido:\n\\(E(\\varepsilon_t) = 0\\) implica que el ruido no afecta la esperanza teórica.\nEstacionariedad en Media:\nSi \\(E(Y_t) = \\mu\\) (constante), la serie es estacionaria en media.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#autocovarianza-de-una-serie-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#autocovarianza-de-una-serie-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.5 Autocovarianza de una serie de Tiempo",
    "text": "2.5 Autocovarianza de una serie de Tiempo\nLa autocovarianza mide la dependencia lineal entre dos observaciones de una serie temporal separadas por un retardo \\(j\\). Para una serie \\(\\{Y_t\\}_{t=0}^\\infty\\), se define como:\n\\[\n\\gamma_j = \\mathbb{E}\\left[(Y_t - \\mu_t)(Y_{t-j} - \\mu_{t-j})\\right]\n\\]\ndonde:\n- \\(\\mu_t = \\mathbb{E}(Y_t)\\): Media de la serie en el tiempo \\(t\\).\n- \\(\\mu_{t-j} = \\mathbb{E}(Y_{t-j})\\): Media en el tiempo \\(t-j\\).\nPara calcular \\(\\gamma_j\\), se construye un vector \\(x_t\\) con las \\(j+1\\) observaciones más recientes:\n\\[\nx_t =\n\\begin{pmatrix}\nY_t \\\\\nY_{t-1} \\\\\n\\vdots \\\\\nY_{t-j}\n\\end{pmatrix}\n\\]\nLa distribución conjunta de \\((Y_t, Y_{t-1}, \\ldots, Y_{t-j})\\) permite derivar las autocovarianzas.\n\n2.5.1 Interpretación\n\nAutocovarianza de orden \\(j\\) (\\(\\gamma_j\\)):\n\nMide cómo \\(Y_t\\) covaría con su valor pasado \\(Y_{t-j}\\).\n\nSi \\(\\gamma_j &gt; 0\\): Valores altos/bajos de \\(Y_t\\) tienden a asociarse con valores altos/bajos de \\(Y_{t-j}\\).\n\nSi \\(\\gamma_j &lt; 0\\): Valores altos de \\(Y_t\\) se vinculan con valores bajos de \\(Y_{t-j}\\), y viceversa.\n\nAutocovarianza de orden 0 (\\(\\gamma_0\\)):\n\\[\n\\gamma_0 = \\mathbb{E}\\left[(Y_t - \\mu_t)^2\\right] = \\text{Var}(Y_t)\n\\]\nEs la varianza de la serie, es decir, la dispersión de \\(Y_t\\) alrededor de su media.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#autcorrelación-de-una-serie-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#autcorrelación-de-una-serie-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.6 Autcorrelación de una serie de Tiempo",
    "text": "2.6 Autcorrelación de una serie de Tiempo\n\n2.6.1 Definición del Coeficiente de Autocorrelación\nEl coeficiente de autocorrelación de orden \\(k\\) para una serie de tiempo \\(\\{Y_t\\}_{t=0}^\\infty\\) se define como:\n\\[\n\\rho_k = \\frac{\\mathbb{E}\\left[(Y_t - \\mu_t)(Y_{t+k} - \\mu_t)\\right]}{\\sqrt{\\mathbb{E}\\left[(Y_t - \\mu_t)^2\\right]\\mathbb{E}\\left[(Y_{t+k} - \\mu_t)^2\\right]}} = \\frac{\\text{Cov}(Y_t, Y_{t+k})}{\\text{Var}(Y_t)} = \\frac{\\gamma_k}{\\gamma_0}\n\\]\n\n\n2.6.2 Función de Autocorrelación (ACF)\nLa función de autocorrelación (ACF) es la colección de valores \\(\\{\\rho_k\\}\\) para \\(k = 0, 1, 2, \\ldots\\).\nPropiedades clave:\n1. Adimensional:\n- \\(\\rho_k\\) no depende de la escala de medición de la serie.\n- Valores en \\([-1, 1]\\).\n2. Simetría:\n- \\(\\rho_k = \\rho_{-k}\\) (simétrica alrededor de \\(k=0\\)).\n3. Interpretación:\n- \\(\\rho_k = 1\\): Correlación perfecta positiva en retardo \\(k\\).\n- \\(\\rho_k = -1\\): Correlación perfecta negativa en retardo \\(k\\).\n\nEjemplo Gráfico de la ACF\n\n\n\n\n2.6.3 Cálculo de la ACF Muestral\nPara una serie de \\(T\\) observaciones, la ACF muestral se estima como:\n\\[\n\\hat{\\rho}_k = \\frac{\\sum_{t=1}^{T-k} (Y_t - \\overline{Y})(Y_{t+k} - \\overline{Y})}{\\sum_{t=1}^T (Y_t - \\overline{Y})^2}, \\quad k = 0, 1, \\ldots, K\n\\]\ndonde \\(\\overline{Y}\\) es la media muestral y \\(K \\approx T/4\\) (regla práctica).\nNota: La ACF es una herramienta fundamental en el análisis de series de tiempo, utilizada en modelos como ARIMA y para diagnósticos de estacionariedad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#autocovarianza-y-autocorrelación-muestral",
    "href": "Caps/02-Series de Tiempo.html#autocovarianza-y-autocorrelación-muestral",
    "title": "2  Series de Tiempo",
    "section": "2.7 Autocovarianza y Autocorrelación Muestral",
    "text": "2.7 Autocovarianza y Autocorrelación Muestral\nEn el análisis de series de tiempo, la estimación de la autocovarianza y autocorrelación muestral es esencial para identificar patrones de dependencia temporal en conjuntos de datos finitos. Para una serie de tiempo \\(Y_1, Y_2, \\ldots, Y_T\\), estas funciones se calculan de la siguiente manera:\n\n2.7.1 Estimación de la Autocovarianza\nLa autocovarianza muestral de orden \\(k\\), denotada como \\(c_k\\), se define mediante:\n\\[\nc_k = \\frac{1}{T} \\sum_{t=1}^{T-k} (Y_t - \\bar{Y})(Y_{t+k} - \\bar{Y}),\n\\]\ndonde \\(\\bar{Y}\\) es la media muestral de la serie. Este estimador utiliza un divisor \\(T\\) (en lugar de \\(T - k\\)) para garantizar que la matriz de autocovarianza sea definida positiva, aunque introduce un ligero sesgo en muestras pequeñas. La simetría de la función (\\(c_k = c_{-k}\\)) facilita su interpretación en aplicaciones prácticas.\n\n\n2.7.2 Función de Autocorrelación Muestral (ACF)\nLa autocorrelación muestral estandariza la autocovarianza para obtener valores adimensionales entre \\(-1\\) y \\(1\\):\n\\[\nr_k = \\frac{c_k}{c_0}, \\quad \\text{con } c_0 = \\frac{1}{T} \\sum_{t=1}^T (Y_t - \\bar{Y})^2.\n\\]\nAquí, \\(c_0\\) corresponde a la varianza muestral. Un valor de \\(r_k\\) cercano a \\(1\\) indica una correlación positiva fuerte en el retardo \\(k\\), mientras que \\(r_k \\approx -1\\) sugiere una correlación negativa. Valores próximos a \\(0\\) implican independencia lineal.\n\n\n2.7.3 Consideraciones para una Estimación Confiable\nPara minimizar errores en la estimación, se recomienda un tamaño muestral mínimo de \\(T \\geq 50\\). Esto reduce la varianza de los estimadores, particularmente en retardos altos. Además, el retardo máximo \\(K\\) suele fijarse como \\(K \\approx T/4\\), equilibrando la captura de patrones relevantes y la evitación de ruido. Por ejemplo, en una serie con \\(T = 100\\), se calcularían autocorrelaciones hasta \\(K = 25\\).\n\n\n2.7.4 Aplicaciones en el Análisis\nLa ACF muestral es fundamental para identificar componentes como estacionalidad (picos en retardos específicos, como \\(k = 12\\) en datos mensuales) o ruido blanco (valores dentro de intervalos de confianza). En modelos ARIMA, los residuos deben exhibir una ACF compatible con ruido blanco para validar su adecuación.\nEsta sección subraya la importancia de estas herramientas en la modelización de series temporales, proporcionando una base para técnicas avanzadas como la descomposición estacional y la predicción.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#autocorrelación-parcial-de-una-serie-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#autocorrelación-parcial-de-una-serie-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.8 Autocorrelación Parcial de una Serie de Tiempo",
    "text": "2.8 Autocorrelación Parcial de una Serie de Tiempo\nLa autocorrelación parcial (PACF, por sus siglas en inglés) es una medida que cuantifica la correlación entre dos observaciones de una serie temporal, \\(Y_t\\) y \\(Y_{t-k}\\), después de eliminar el efecto lineal de las observaciones intermedias \\(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-k+1}\\). Este concepto se basa en el principio de correlación parcial entre variables aleatorias, ajustando por factores de confusión.\n\n2.8.1 Fundamentos Teóricos\nSean \\(X\\), \\(Y\\), y \\(Z\\) variables aleatorias. La correlación parcial entre \\(X\\) e \\(Y\\), ajustando por \\(Z\\), se obtiene mediante los siguientes pasos:\n1. Regresión lineal de \\(X\\) sobre \\(Z\\):\n\\[\n   \\hat{X} = a_1 + b_1 Z, \\quad b_1 = \\frac{\\text{Cov}(Z, X)}{\\text{Var}(Z)}.\n   \\]\n2. Regresión lineal de \\(Y\\) sobre \\(Z\\):\n\\[\n   \\hat{Y} = a_2 + b_2 Z, \\quad b_2 = \\frac{\\text{Cov}(Z, Y)}{\\text{Var}(Z)}.\n   \\]\n3. Residuos:\n\\[\n   X^* = X - \\hat{X}, \\quad Y^* = Y - \\hat{Y}.\n   \\]\nLa correlación parcial entre \\(X\\) e \\(Y\\) (ajustando por \\(Z\\)) es la correlación entre los residuos \\(X^*\\) y \\(Y^*\\).\n\n\n2.8.2 Aplicación a Series de Tiempo\nPara una serie temporal \\(\\{Y_t\\}\\), la autocorrelación parcial en el retardo \\(k\\) se define como la correlación entre \\(Y_t\\) y \\(Y_{t-k}\\), tras eliminar la influencia de \\(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-k+1}\\). Matemáticamente:\n\\[\n\\text{PACF}(k) = \\text{Corr}(Y_t^*,\\, Y_{t-k}^*),\n\\]\ndonde \\(Y_t^*\\) y \\(Y_{t-k}^*\\) son los residuos de regresiones lineales que ajustan por los rezagos intermedios.\n\nInterpretación\n\n\\(\\text{PACF}(k) \\neq 0\\): Indica una dependencia directa entre \\(Y_t\\) y \\(Y_{t-k}\\) no mediada por rezagos anteriores.\n\n\\(\\text{PACF}(k) \\approx 0\\): Sugiere que la correlación en el retardo \\(k\\) se explica completamente por los rezagos intermedios.\n\n\n\n\n2.8.3 Relevancia en Modelado\nLa PACF es crítica en modelos autorregresivos (AR). Por ejemplo, en un proceso AR(p):\n- La PACF tiene cortes abruptos en el retardo \\(p\\).\n- Los valores significativos en \\(\\text{PACF}(k)\\) para \\(k \\leq p\\) ayudan a determinar el orden del modelo.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#definición-formal-de-la-autocorrelación-parcial-y-ecuaciones-de-yule-walker",
    "href": "Caps/02-Series de Tiempo.html#definición-formal-de-la-autocorrelación-parcial-y-ecuaciones-de-yule-walker",
    "title": "2  Series de Tiempo",
    "section": "2.9 Definición Formal de la Autocorrelación Parcial y Ecuaciones de Yule-Walker",
    "text": "2.9 Definición Formal de la Autocorrelación Parcial y Ecuaciones de Yule-Walker\n\n2.9.1 Ecuaciones de Yule-Walker para Procesos AR\nPara un modelo de serie de tiempo estacionaria \\(\\{Y_i\\}_{i \\geq 0}\\), las ecuaciones de Yule-Walker relacionan los coeficientes autorregresivos (\\(\\phi_{ik}\\)) con la función de autocorrelación (\\(\\rho(j)\\)). Para un orden fijo \\(k\\), estas ecuaciones se expresan como:\n\\[\n\\rho(j) = \\sum_{i=1}^k \\phi_{ik} \\rho(j-i), \\quad j = 1, 2, \\ldots, k,\n\\]\nque, expandidas, toman la forma:\n\\[\n\\begin{aligned}\n\\rho(1) &= \\phi_{1k} + \\phi_{2k}\\rho(1) + \\cdots + \\phi_{kk}\\rho(k-1), \\\\\n\\rho(2) &= \\phi_{1k}\\rho(1) + \\phi_{2k} + \\cdots + \\phi_{kk}\\rho(k-2), \\\\\n&\\;\\;\\vdots \\\\\n\\rho(k) &= \\phi_{1k}\\rho(k-1) + \\phi_{2k}\\rho(k-2) + \\cdots + \\phi_{kk}.\n\\end{aligned}\n\\]\n\n\n2.9.2 Notación Matricial\nEl sistema de ecuaciones se representa matricialmente como:\n\\[\n\\begin{pmatrix}\n1 & \\rho(1) & \\rho(2) & \\cdots & \\rho(k-1) \\\\\n\\rho(1) & 1 & \\rho(1) & \\cdots & \\rho(k-2) \\\\\n\\rho(2) & \\rho(1) & 1 & \\cdots & \\rho(k-3) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho(k-1) & \\rho(k-2) & \\rho(k-3) & \\cdots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\phi_{1k} \\\\\n\\phi_{2k} \\\\\n\\phi_{3k} \\\\\n\\vdots \\\\\n\\phi_{kk}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\rho(1) \\\\\n\\rho(2) \\\\\n\\rho(3) \\\\\n\\vdots \\\\\n\\rho(k)\n\\end{pmatrix},\n\\]\no, en forma compacta:\n\\[\nP_k \\phi_k = \\rho_k,\n\\]\ndonde \\(P_k\\) es la matriz de autocorrelaciones y \\(\\rho_k\\) es el vector de autocorrelaciones hasta el retardo \\(k\\).\n\n\n\n2.9.3 Solución y Autocorrelación Parcial\nLos coeficientes \\(\\phi_k\\) se obtienen invirtiendo la matriz \\(P_k\\):\n\\[\n\\phi_k = P_k^{-1} \\rho_k.\n\\]\nEl último coeficiente del vector \\(\\phi_k\\), denotado \\(\\phi_{kk}\\), corresponde a la autocorrelación parcial en el retardo \\(k\\).\n\nInterpretación de \\(\\phi_{kk}\\)\n\n\\(\\phi_{kk} \\neq 0\\): Indica una dependencia directa entre \\(Y_t\\) y \\(Y_{t-k}\\) no explicada por los rezagos intermedios.\n\n\\(\\phi_{kk} \\approx 0\\): La correlación en el retardo \\(k\\) se debe completamente a los rezagos anteriores.\n\n\n\n\n2.9.4 Ejemplo: Proceso AR(2)\nPara un modelo AR(2) (\\(k=2\\)):\n\\[\n\\phi_{22} = \\frac{\\rho(2) - \\rho(1)^2}{1 - \\rho(1)^2}.\n\\]\nEste valor representa la autocorrelación parcial en el retardo 2, crucial para identificar el orden del modelo.\n\n\n2.9.5 Aplicación en Identificación de Modelos\n\nCorte en la PACF: En procesos AR(p), la PACF tiene valores significativos hasta el retardo \\(p\\) y luego cae abruptamente.\n\n\nEsta metodología es fundamental para seleccionar órdenes en modelos autorregresivos y validar supuestos de estacionariedad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#estacionariedad-en-series-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#estacionariedad-en-series-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.10 Estacionariedad en Series de Tiempo",
    "text": "2.10 Estacionariedad en Series de Tiempo\nUn proceso \\(\\{Y_t\\}\\) se define como estacionario en covarianza (o débilmente estacionario) cuando satisface dos condiciones fundamentales:\n\n2.10.1 Condiciones para la estacionariedad débil\n1. Media constante\nLa esperanza matemática de \\(Y_t\\) permanece invariante en el tiempo:\n\\[\n\\mathbb{E}(Y_t) = \\mu \\quad \\text{para todo } t.\n\\]\nPor ejemplo, en una serie de ventas mensuales estacionaria, no existirían tendencias ascendentes o descendentes a largo plazo, manteniendo un promedio estable.\n2. Autocovarianzas dependientes del lag\nLa covarianza entre \\(Y_t\\) y \\(Y_{t-j}\\) solo depende del desplazamiento temporal \\(j\\), no del instante \\(t\\):\n\\[\n\\mathbb{E}[(Y_t - \\mu)(Y_{t-j} - \\mu)] = \\gamma_j \\quad \\text{para todo } t \\text{ y cualquier } j.\n\\]\nEsto implica que la relación entre observaciones separadas por \\(j\\) periodos es constante. Por ejemplo, la correlación entre las ventas de enero y febrero sería idéntica a la de febrero y marzo si \\(j=1\\).\n\n\n2.10.2 Prueba de Dickey-Fuller: Validando la estacionariedad\nEsta prueba estadística detecta la presencia de una raíz unitaria, señal de no estacionariedad. Su interpretación es clave:\n- Si el estadístico de prueba es menor que los valores críticos (1%, 5%, 10%), se rechaza la hipótesis nula de raíz unitaria.\n- Esto confirma que la serie es estacionaria, permitiendo modelarla con técnicas como ARMA o ARIMA.\n\n\n2.10.3 Técnicas para lograr estacionariedad\nDiferenciación\nTransformar la serie calculando la diferencia entre observaciones consecutivas:\n\\[\n\\Delta Y_t = Y_t - Y_{t-1}.\n\\]\nEsto elimina tendencias deterministas y reduce la dependencia temporal.\nTransformaciones no lineales\n- Aplicar logaritmos: \\(\\log(Y_t)\\) para estabilizar varianzas.\n- Diferenciación estacional: \\(\\Delta_{12} Y_t = Y_t - Y_{t-12}\\) en datos mensuales con patrones anuales.\n\n\n2.10.4 Estacionariedad débil vs. estricta\nEstacionariedad débil\nRequiere media constante y autocovarianzas que dependen solo del lag \\(j\\). Es la base de modelos como ARMA, al ser menos restrictiva y más aplicable en la práctica.\nEstacionariedad estricta\nExige que todas las distribuciones conjuntas (no solo media y covarianza) sean invariantes en el tiempo. Es teóricamente robusta pero raramente verificable en datos reales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/02-Series de Tiempo.html#ejemplos-ilustrativos-de-estacionariedad-en-series-de-tiempo",
    "href": "Caps/02-Series de Tiempo.html#ejemplos-ilustrativos-de-estacionariedad-en-series-de-tiempo",
    "title": "2  Series de Tiempo",
    "section": "2.11 Ejemplos ilustrativos de estacionariedad en series de tiempo",
    "text": "2.11 Ejemplos ilustrativos de estacionariedad en series de tiempo\n\n2.11.1 Caso estacionario\nModelo:\n\\[\nY_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2)\n\\]\nPropiedades:\n1. Media constante:\n\\[\n   \\mathbb{E}(Y_t) = \\mu \\quad \\text{para todo } t.\n   \\]\nLa ausencia de términos dependientes de \\(t\\) garantiza que la media no varíe con el tiempo.\n\nAutocovarianzas:\n\\[\n\\gamma_j = \\mathbb{E}[(Y_t - \\mu)(Y_{t-j} - \\mu)] =\n\\begin{cases}\n\\sigma^2 & \\text{si } j = 0 \\\\\n0 & \\text{si } j \\neq 0\n\\end{cases}.\n\\]\nLas autocovarianzas solo dependen del lag \\(j\\), no de \\(t\\).\n\n\n\n2.11.2 Caso no estacionario\nModelo:\n\\[\nY_t = \\beta t + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2)\n\\]\nPropiedades:\n1. Media variable:\n\\[\n   \\mathbb{E}(Y_t) = \\beta t.\n   \\]\nLa presencia del término \\(\\beta t\\) introduce una tendencia lineal, haciendo que la media crezca con \\(t\\).\n\nAutocovarianzas:\nAunque las autocovarianzas para \\(j \\neq 0\\) son cero (como en el caso estacionario), la dependencia temporal de la media viola la condición de estacionariedad.\n\n\n\n2.11.3 Comparación clave\n\n\n\n\n\n\n\n\nCaracterística\nCaso estacionario\nCaso no estacionario\n\n\n\n\nMedia\nConstante (\\(\\mu\\))\nDepende de \\(t\\) (\\(\\beta t\\))\n\n\nAutocovarianzas\nDependen solo de \\(j\\)\nDependen de \\(j\\), pero media variable\n\n\nTendencia\nAusente\nLineal (\\(\\beta t\\))\n\n\n\nImportancia práctica:\n- Los modelos como ARMA requieren estacionariedad. Si una serie tiene tendencia (como el segundo caso), se debe aplicar diferenciación:\n\\[\n  \\Delta Y_t = Y_t - Y_{t-1} \\quad \\text{(elimina tendencias deterministas)}.\n  \\]\n- La prueba de Dickey-Fuller ayuda a detectar no estacionariedad causada por raíces unitarias o tendencias.\n\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Series de Tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "",
    "text": "3.1 Procesos de Media Movil (MA)\nUn proceso de media móvil de orden \\(q\\), denotado como MA(\\(q\\)), modela una serie temporal donde el valor actual depende linealmente de los errores aleatorios ocurridos en los últimos \\(q\\) periodos. Su forma general es:\n\\[\nY_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\cdots + \\theta_q \\epsilon_{t-q}\n\\]\nAquí \\(\\mu\\) es la media constante del proceso. - \\(\\epsilon_t, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-q}\\): Términos de error (ruido blanco) independientes e idénticamente distribuidos (i.i.d.), con distribución \\(\\epsilon_t \\sim N(0, \\sigma^2)\\). - \\(\\theta_1, \\theta_2, \\ldots, \\theta_q\\): Coeficientes reales que miden el impacto de los errores pasados en el valor actual.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-media-movil-ma",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-media-movil-ma",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "",
    "text": "3.1.1 Propiedades:\n\nPersistencia del ruido:\nEl efecto de un shock (\\(\\epsilon_t\\)) no desaparece inmediatamente. En su lugar, influye en las observaciones durante \\(q\\) periodos consecutivos.\nEjemplo: En un modelo MA(2), un error en el tiempo \\(t\\) afecta a \\(Y_t\\), \\(Y_{t+1}\\), y \\(Y_{t+2}\\).\nDecaimiento abrupto:\nDespués de \\(q\\) periodos, el impacto del ruido se anula por completo. Esto contrasta con modelos autorregresivos (AR), donde los efectos pueden persistir indefinidamente.\nEjemplo: En un MA(3), el error \\(\\epsilon_{t-3}\\) no influye en \\(Y_t\\), ya que su efecto termina en \\(t = 3\\).\n\n\n\n3.1.2 Modelización con MA(2)\nEcuación del modelo:\n\\[\nY_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2}\n\\]\nInterpretación:\n- \\(Y_t\\): Demanda en el día \\(t\\).\n- \\(\\mu\\): Demanda base constante (sin ofertas).\n- \\(\\epsilon_t\\): Shock por oferta especial en el día \\(t\\) (ruido blanco \\(\\sim N(0, \\sigma^2)\\)).\n- \\(\\theta_1, \\theta_2\\): Coeficientes que miden el impacto residual de la oferta en los días posteriores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-modelos-maq",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-modelos-maq",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.2 Características de los Modelos MA(q)",
    "text": "3.2 Características de los Modelos MA(q)\n\n3.2.1 Valor esperado\nUn proceso MA(\\(q\\)) tiene una media constante, lo que garantiza estacionariedad en media. Esto se deriva de la linealidad del operador esperanza y la propiedad de media cero de los errores \\(\\varepsilon_t\\):\n\\[\n\\mathbb{E}(Y_t) = \\mu + \\mathbb{E}(\\varepsilon_t) + \\theta_1 \\mathbb{E}(\\varepsilon_{t-1}) + \\cdots + \\theta_q \\mathbb{E}(\\varepsilon_{t-q}) = \\mu.\n\\]\nExplicación:\n- Los términos \\(\\varepsilon_{t-j}\\) son ruido blanco, por lo que \\(\\mathbb{E}(\\varepsilon_{t-j}) = 0\\) para todo \\(j\\).\n- La constante \\(\\mu\\) representa el nivel base de la serie, independiente del tiempo.\n\n\n3.2.2 Varianza\nLa varianza de un proceso MA(\\(q\\)) refleja la dispersión alrededor de la media \\(\\mu\\) y depende de los coeficientes \\(\\theta_j\\) y la varianza del ruido (\\(\\sigma^2\\)):\n\\[\n\\gamma_0 = \\mathbb{E}[(Y_t - \\mu)^2] = \\mathbb{E}\\left[(\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\cdots + \\theta_q \\varepsilon_{t-q})^2\\right].\n\\]\nDado que los errores \\(\\varepsilon_t\\) son no correlacionados (\\(\\mathbb{E}[\\varepsilon_t \\varepsilon_{t-j}] = 0\\) para \\(j \\neq 0\\)), la expresión se simplifica a:\n\\[\n\\gamma_0 = \\sigma^2 \\left(1 + \\theta_1^2 + \\theta_2^2 + \\cdots + \\theta_q^2\\right).\n\\]\nInterpretación:\n- Cada término \\(\\theta_j^2 \\sigma^2\\) captura la contribución del error rezagado \\(j\\) a la varianza total.\n- La varianza es siempre positiva y aumenta con la magnitud de los coeficientes \\(\\theta_j\\).\n\n\n3.2.3 Implicaciones clave\n\nEstacionariedad en covarianza:\n\nLa media constante (\\(\\mu\\)) y la varianza finita (\\(\\gamma_0\\)) garantizan que el proceso MA(\\(q\\)) sea débilmente estacionario.\n\nLas autocovarianzas \\(\\gamma_j\\) para \\(j &gt; q\\) son cero, lo que simplifica el análisis de dependencias temporales.\n\nEfecto de los coeficientes \\(\\theta_j\\):\n\nCoeficientes mayores en magnitud aumentan la volatilidad de la serie (\\(\\uparrow \\gamma_0\\)).\n\nUn \\(\\theta_j = 0\\) implica que el error en el lag \\(j\\) no influye en \\(Y_t\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#función-de-autocovarianza-de-los-modelos-maq",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#función-de-autocovarianza-de-los-modelos-maq",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.3 Función de autocovarianza de los modelos MA(q)",
    "text": "3.3 Función de autocovarianza de los modelos MA(q)\n\n3.3.1 Autocovarianza de un proceso MA(q)\nLa j-ésima autocovarianza (\\(\\gamma_j\\)) de un proceso de media móvil de orden \\(q\\), MA(\\(q\\)), cuantifica la dependencia lineal entre observaciones separadas por \\(j\\) periodos. Para \\(j = 1, 2, \\ldots, q\\), se define como:\n\\[\n\\gamma_j = \\mathbb{E}\\left[ \\left( \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots + \\theta_q \\varepsilon_{t-q} \\right) \\cdot \\left( \\varepsilon_{t-j} + \\theta_1 \\varepsilon_{t-j-1} + \\cdots + \\theta_q \\varepsilon_{t-j-q} \\right) \\right].\n\\]\nDado que los términos de error \\(\\{\\varepsilon_t\\}\\) son no correlacionados (\\(\\mathbb{E}[\\varepsilon_t \\varepsilon_{s}] = 0\\) para \\(t \\neq s\\)), solo sobreviven los productos donde los índices coinciden. Esto simplifica la expresión a:\n\\[\n\\gamma_j = \\sigma^2 \\left( \\theta_j + \\theta_{j+1} \\theta_1 + \\theta_{j+2} \\theta_2 + \\cdots + \\theta_q \\theta_{q-j} \\right), \\quad \\text{para } j \\leq q.\n\\]\nPara \\(j &gt; q\\), no hay superposición entre los términos de error, por lo que:\n\\[\n\\gamma_j = 0 \\quad \\text{si } j &gt; q.\n\\]\n\n\n3.3.2 Autocorrelación de un proceso MA(q)\nLa j-ésima autocorrelación (\\(\\rho_j\\)) normaliza la autocovarianza respecto a la varianza del proceso (\\(\\gamma_0\\)):\n\\[\n\\rho_j = \\frac{\\gamma_j}{\\gamma_0} = \\frac{\\theta_j + \\theta_{j+1} \\theta_1 + \\theta_{j+2} \\theta_2 + \\cdots + \\theta_q \\theta_{q-j}}{1 + \\theta_1^2 + \\theta_2^2 + \\cdots + \\theta_q^2}, \\quad \\text{para } j \\leq q.\n\\]\nPara \\(j &gt; q\\), la autocorrelación es cero:\n\\[\n\\rho_j = 0 \\quad \\text{si } j &gt; q.\n\\]\n\n\n3.3.3 Casos particulares\n\nProceso MA(1)\nUn proceso de media móvil de orden 1, MA(1), se define como:\n\\[\nY_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1},\n\\]\ndonde \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\) es ruido blanco.\n\nAutocovarianzas del MA(1):\n\nVarianza (\\(\\gamma_0\\)):\n\\[\n\\gamma_0 = \\left(1 + \\theta_1^2\\right) \\sigma^2.\n\\]\nMide la dispersión total alrededor de la media \\(\\mu\\), incluyendo el efecto del shock actual (\\(\\varepsilon_t\\)) y el rezagado (\\(\\theta_1 \\varepsilon_{t-1}\\)).\nAutocovarianza de primer orden (\\(\\gamma_1\\)):\n\\[\n\\gamma_1 = \\theta_1 \\sigma^2.\n\\]\nCaptura la dependencia entre observaciones separadas por un periodo.\nAutocovarianzas para \\(j \\geq 2\\):\n\\[\n\\gamma_j = 0 \\quad \\text{para } j \\geq 2.\n\\]\nLos shocks no persisten más allá de un periodo, característica clave de los MA(1).\n\n\n\n\n\nProceso MA(2)\nUn proceso de media móvil de orden 2, MA(2), se expresa como:\n\\[\nY_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2}.\n\\]\n\nAutocovarianzas del MA(2):\n\nVarianza (\\(\\gamma_0\\)):\n\\[\n\\gamma_0 = \\left(1 + \\theta_1^2 + \\theta_2^2\\right) \\sigma^2.\n\\]\nIncluye la contribución de los shocks actuales y los dos rezagos anteriores.\nAutocovarianza de primer orden (\\(\\gamma_1\\)):\n\\[\n\\gamma_1 = \\left(\\theta_1 + \\theta_1 \\theta_2\\right) \\sigma^2.\n\\]\nRefleja la influencia combinada del shock en \\(t-1\\) y su interacción con \\(t-2\\).\nAutocovarianza de segundo orden (\\(\\gamma_2\\)):\n\\[\n\\gamma_2 = \\theta_2 \\sigma^2.\n\\]\nRepresenta el impacto directo del shock en \\(t-2\\).\nAutocovarianzas para \\(j \\geq 3\\):\n\\[\n\\gamma_j = 0 \\quad \\text{para } j \\geq 3.\n\\]\nLos efectos de los shocks se limitan a dos periodos.\n\n\n\n\nEjemplo\n\n\nCódigo\n##### PRIMER EJEMPLO MA(1)\n\nset.seed(1234)\n\n# Simular un MA(1): Y_t = e_t + 0.7*e_{t-1}\nma1 = arima.sim(n = 200, list(ma = 0.7))\n\n# Graficar la serie\nts.plot(ma1, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma (ACF)\nacf(ma1, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma parcial (PACF)\npacf(ma1, main = \"PACF\")\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\nCódigo\n### SEGUNDO EJEMPLO MA(3)\n\n# Simular un MA(3): Y_t = e_t + 0.6*e_{t-1}\n                        # - 0.4*e_{t-2} + 0.3*e_{t-3}\nma3 &lt;- arima.sim(n = 200, list(ma = c(0.6, -0.4, 0.3)))\n\n# Graficar la serie\nts.plot(ma3, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma (ACF)\nacf(ma3, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma parcial (PACF)\npacf(ma3, main = \"PACF\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#ma-de-orden-infinito",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#ma-de-orden-infinito",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.4 MA de orden infinito",
    "text": "3.4 MA de orden infinito\nUn proceso de media móvil de orden infinito, denotado como \\(MA(\\infty)\\), modela series temporales donde el valor actual depende de una combinación lineal infinita de errores pasados. Su estructura matemática se define como:\n\\[\nY_t = \\mu + \\sum_{j=0}^\\infty \\psi_j \\epsilon_{t-j} = \\mu + \\epsilon_t + \\psi_1 \\epsilon_{t-1} + \\psi_2 \\epsilon_{t-2} + \\cdots\n\\]\n\n3.4.1 Componentes clave:\n\n\\(\\mu\\): Media constante del proceso.\n\n\\(\\epsilon_t, \\epsilon_{t-1}, \\epsilon_{t-2}, \\ldots\\): Términos de error independientes e idénticamente distribuidos (i.i.d.), con distribución \\(\\epsilon_t \\sim N(0, \\sigma^2)\\).\n\n\\(\\psi_j\\): Coeficientes reales que ponderan el impacto de los errores pasados en \\(Y_t\\).\n\n\n\n3.4.2 Características principales:\n\nPersistencia infinita del ruido:\nA diferencia de los modelos MA(\\(q\\)), donde los shocks desaparecen después de \\(q\\) periodos, en un \\(MA(\\infty)\\) el efecto de un error \\(\\epsilon_t\\) perdura indefinidamente. Cada shock influye en todas las observaciones futuras, aunque su impacto disminuye según la magnitud de los coeficientes \\(\\psi_j\\).\nCondición de estacionariedad:\nPara garantizar que el proceso sea estacionario en covarianza, los coeficientes deben satisfacer:\n\\[\n\\sum_{j=0}^\\infty \\psi_j^2 &lt; \\infty.\n\\]\nEsta condición asegura que la varianza del proceso sea finita, evitando que las contribuciones acumuladas de los errores pasados divergían.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-procesos-mainfty",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-procesos-mainfty",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.5 Características de los procesos MA(infty)",
    "text": "3.5 Características de los procesos MA(infty)\n\n3.5.1 Valor esperado\nLa media de un proceso \\(MA(\\infty)\\) se mantiene constante en el tiempo, independientemente de los errores pasados. Esto se deriva directamente de la linealidad del operador esperanza y la propiedad de media cero del ruido blanco \\(\\epsilon_t\\):\n\\[\n\\mathbb{E}(Y_t) = \\mu.\n\\]\nEsta condición garantiza que el proceso sea estacionario en media, un requisito fundamental para su análisis estadístico.\n\n\n3.5.2 Varianza\nLa varianza del proceso \\(MA(\\infty)\\) se calcula considerando la contribución acumulada de todos los términos de error pasados. Dado que los coeficientes \\(\\psi_j\\) ponderan estos errores, la varianza queda expresada como:\n\\[\n\\gamma_0 = \\mathbb{E}[(Y_t - \\mu)^2] = \\sigma^2 \\sum_{j=0}^{\\infty} \\psi_j^2.\n\\]\nPara que esta varianza sea finita (y el proceso sea estacionario en covarianza), la serie de los cuadrados de los coeficientes debe converger:\n\\[\n\\sum_{j=0}^{\\infty} \\psi_j^2 &lt; \\infty.\n\\]\nEsta condición asegura que los efectos de los shocks pasados no se acumulen indefinidamente, evitando que la varianza diverja.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#importancia-de-los-procesos-ma",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#importancia-de-los-procesos-ma",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.6 Importancia de los procesos MA(∞)",
    "text": "3.6 Importancia de los procesos MA(∞)\nLa representación de media móvil infinita (MA(∞)) es un marco teórico fundamental para analizar series temporales estacionarias. Su relevancia radica en su capacidad para modelar cualquier proceso estacionario no determinístico, una afirmación respaldada por el Teorema de Wold (1938).\n\n3.6.1 Teorema de Wold: La base teórica\nEste teorema establece que toda serie temporal estacionaria no determinística puede descomponerse en dos componentes:\n1. Una parte determinística (por ejemplo, tendencias o ciclos perfectamente predecibles).\n2. Una parte no determinística, expresable como una combinación lineal infinita de errores pasados:\n\\[\ny_t = \\mu + \\sum_{i=0}^{\\infty} \\psi_i e_{t-i},\n\\]\ndonde:\n- \\(e_t\\) es ruido blanco (\\(e_t \\sim N(0, \\sigma^2)\\)).\n- Los coeficientes \\(\\psi_i\\) satisfacen \\(\\sum_{i=0}^{\\infty} \\psi_i^2 &lt; \\infty\\), garantizando estacionariedad en covarianza.\n\n\n3.6.2 Casos especiales de la representación MA(∞)\nAunque el Teorema de Wold es general, en la práctica se utilizan modelos más simples que son subclases de MA(∞):\n\nModelos MA(\\(q\\)):\nSolo un número finito de coeficientes \\(\\psi_i\\) son no nulos. Por ejemplo, en un MA(2), \\(\\psi_0 = 1\\), \\(\\psi_1 = \\theta_1\\), \\(\\psi_2 = \\theta_2\\), y \\(\\psi_j = 0\\) para \\(j &gt; 2\\).\nModelos AR(\\(p\\)):\nLos coeficientes \\(\\psi_j\\) se generan recursivamente a partir de un número finito de parámetros autorregresivos. Por ejemplo, un AR(1) con parámetro \\(\\phi\\) tiene \\(\\psi_j = \\phi^j\\), lo que implica una dependencia exponencial decreciente.\nModelos ARMA(\\(p, q\\)):\nCombina componentes AR y MA finitos, permitiendo mayor flexibilidad al capturar tanto dependencias autorregresivas como shocks transitorios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-autorregresión-de-orden-1-ar",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-autorregresión-de-orden-1-ar",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.7 Procesos de autorregresión de orden 1 (AR)",
    "text": "3.7 Procesos de autorregresión de orden 1 (AR)\nUn proceso AR(1) se define mediante la ecuación:\n\\[\nY_t = c + \\phi Y_{t-1} + \\epsilon_t,\n\\]\ndonde:\n- \\(\\epsilon_t \\sim N(0, 1)\\): Ruido blanco gaussiano.\n- \\(c\\): Término constante (nivel base de la serie).\n- \\(\\phi\\): Coeficiente autorregresivo que mide la influencia del valor pasado \\(Y_{t-1}\\) en \\(Y_t\\).\n\n3.7.1 Condición de Estacionariedad\nEl proceso AR(1) es estacionario en covarianza si y solo si:\n\\[\n|\\phi| &lt; 1.\n\\]\nInterpretación:\n- Si \\(|\\phi| \\geq 1\\), la varianza del proceso diverge (no estacionario).\n- Ejemplo: Si \\(\\phi = 1\\), el modelo se convierte en un paseo aleatorio: \\(Y_t = c + Y_{t-1} + \\epsilon_t\\), con varianza creciente en el tiempo.\n\n\n3.7.2 Propiedades Estadísticas\n\nMedia\nLa media del proceso estacionario es constante:\n\\[\n\\mu = \\frac{c}{1 - \\phi}.\n\\]\nDerivación:\n\\[\n\\mathbb{E}(Y_t) = c + \\phi \\mathbb{E}(Y_{t-1}) \\implies \\mu = c + \\phi \\mu \\implies \\mu = \\frac{c}{1 - \\phi}.\n\\]\n\n\nVarianza\nLa varianza está dada por:\n\\[\n\\sigma^2 = \\frac{1}{1 - \\phi^2}.\n\\]\nDerivación:\n\\[\n\\text{Var}(Y_t) = \\phi^2 \\text{Var}(Y_{t-1}) + \\text{Var}(\\epsilon_t) \\implies \\sigma^2 = \\phi^2 \\sigma^2 + 1 \\implies \\sigma^2 = \\frac{1}{1 - \\phi^2}.\n\\]\n\n\nAutocorrelaciones\nLa autocorrelación en el lag \\(k\\) decae exponencialmente:\n\\[\n\\rho_k = \\phi^k \\quad (k \\geq 0).\n\\]\n- ACF (Función de Autocorrelación): Decae lentamente, mostrando dependencias persistentes.\n- PACF (Función de Autocorrelación Parcial): Tiene un pico significativo en \\(k=1\\) y es cero para \\(k &gt; 1\\).\n\n\n\n3.7.3 Representación MA(∞) bajo estacionariedad\nCuando \\(|\\phi| &lt; 1\\), el proceso AR(1) puede expresarse como una combinación lineal infinita de shocks pasados, es decir, un MA(∞). Esto se demuestra expandiendo recursivamente la ecuación original:\n\\[\n\\begin{aligned}\nY_t &= c + \\phi Y_{t-1} + \\varepsilon_t \\\\\n&= c + \\phi (c + \\phi Y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\\\\n&= c + \\phi c + \\phi^2 Y_{t-2} + \\phi \\varepsilon_{t-1} + \\varepsilon_t \\\\\n&\\;\\;\\vdots \\\\\n&= \\sum_{k=0}^{\\infty} \\phi^k c + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}.\n\\end{aligned}\n\\]\nLa convergencia de estas series está garantizada por la condición \\(|\\phi| &lt; 1\\). Simplificando:\n\\[\nY_t = \\frac{c}{1 - \\phi} + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}.\n\\]\n\n\n\n3.7.4 Componentes clave de la representación:\n\nMedia del proceso:\n\\[\n\\mu = \\frac{c}{1 - \\phi}.\n\\]\nCorresponde al primer término de la serie geométrica convergente.\nEstructura de dependencia:\nLos shocks pasados \\(\\varepsilon_{t-j}\\) influyen en \\(Y_t\\) con pesos \\(\\phi^j\\), que decaen exponencialmente. Esto refleja cómo el impacto de un shock se disipa gradualmente en el tiempo.\nVínculo con estacionariedad:\nLa convergencia \\(\\sum_{j=0}^{\\infty} \\phi^{2j} &lt; \\infty\\) asegura que la varianza sea finita, cumpliendo con la condición de estacionariedad en covarianza.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-procesos-ar1",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#características-de-los-procesos-ar1",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.8 Características de los procesos AR(1)",
    "text": "3.8 Características de los procesos AR(1)\n\n3.8.1 Valor esperado\nPara un proceso AR(1) estacionario (\\(|\\phi| &lt; 1\\)), la media se obtiene aplicando el operador esperanza a su representación MA(∞):\n\\[\n\\mathbb{E}(Y_t) = \\mathbb{E}\\left[ \\frac{c}{1 - \\phi} + \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j} \\right].\n\\]\nDado que \\(\\mathbb{E}(\\varepsilon_{t-j}) = 0\\) para todo \\(j\\), solo sobrevive el término constante:\n\\[\n\\mu = \\frac{c}{1 - \\phi}.\n\\]\nEsto confirma que el proceso tiene media constante, cumpliendo con la estacionariedad en media.\n\n\n3.8.2 Varianza\nLa varianza del proceso se calcula a partir de la desviación respecto a la media:\n\\[\n\\gamma_0 = \\mathbb{E}\\left[(Y_t - \\mu)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\\right)^2\\right].\n\\]\nExpandimos el cuadrado y utilizamos la independencia de los términos \\(\\varepsilon_{t-j}\\):\n\\[\n\\gamma_0 = \\sum_{j=0}^{\\infty} \\phi^{2j} \\mathbb{E}(\\varepsilon_{t-j}^2) = \\sigma^2 \\sum_{j=0}^{\\infty} \\phi^{2j}.\n\\]\nLa serie geométrica \\(\\sum_{j=0}^{\\infty} \\phi^{2j}\\) converge bajo \\(|\\phi| &lt; 1\\), resultando en:\n\\[\n\\gamma_0 = \\frac{\\sigma^2}{1 - \\phi^2}.\n\\]\nEsta expresión refleja cómo la varianza del proceso depende tanto de la volatilidad del ruido (\\(\\sigma^2\\)) como de la persistencia autorregresiva (\\(\\phi\\)).\nCondición clave:\n- \\(|\\phi| &lt; 1\\) asegura convergencia de la serie y estacionariedad en covarianza.\n- Si \\(|\\phi| \\geq 1\\), la varianza diverge, y el proceso no es estacionario.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#función-de-autocovarianza-de-los-procesos-ar1",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#función-de-autocovarianza-de-los-procesos-ar1",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.9 Función de autocovarianza de los procesos AR(1)",
    "text": "3.9 Función de autocovarianza de los procesos AR(1)\n\n3.9.1 Autocovarianza (\\(\\gamma_j\\))\nPara un proceso AR(1) estacionario (\\(|\\phi| &lt; 1\\)), la j-ésima autocovarianza se deriva utilizando su representación MA(∞):\n\\[\nY_t - \\mu = \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k}.\n\\]\nLa autocovarianza se calcula como:\n\\[\n\\gamma_j = \\mathbb{E}\\left[ \\left( \\sum_{k=0}^\\infty \\phi^k \\varepsilon_{t-k} \\right) \\left( \\sum_{m=0}^\\infty \\phi^m \\varepsilon_{t-j-m} \\right) \\right].\n\\]\nDebido a la independencia de los términos \\(\\varepsilon_t\\), solo sobreviven los productos donde los índices coinciden (\\(k = m + j\\)):\n\\[\n\\gamma_j = \\sum_{k=0}^\\infty \\phi^{k+j} \\phi^k \\sigma^2 = \\phi^j \\sigma^2 \\sum_{k=0}^\\infty \\phi^{2k}.\n\\]\nLa serie geométrica converge bajo \\(|\\phi| &lt; 1\\), resultando en:\n\\[\n\\gamma_j = \\frac{\\phi^j \\sigma^2}{1 - \\phi^2} \\quad \\text{para } j \\geq 0.\n\\]\n\n\n3.9.2 Autocorrelación (\\(\\rho_j\\))\nLa j-ésima autocorrelación se obtiene normalizando la autocovarianza por la varianza (\\(\\gamma_0\\)):\n\\[\n\\rho_j = \\frac{\\gamma_j}{\\gamma_0} = \\frac{\\frac{\\phi^j \\sigma^2}{1 - \\phi^2}}{\\frac{\\sigma^2}{1 - \\phi^2}} = \\phi^j.\n\\]\nEsto implica que las autocorrelaciones decaen exponencialmente con el lag \\(j\\):\n\\[\n\\rho_j = \\phi^j \\quad \\text{para } j \\geq 0.\n\\]\n\nEjemplo\n\n\nCódigo\n## PRIMER EJEMPLO AR(1)\n\n# Simular un AR(1): Y_t = 0.5*Y_{t-1} + e_t\nar1 &lt;- arima.sim(n = 200, list(ar = 0.5))\n\n# Graficar la serie\nts.plot(ar1, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma (ACF)\nacf(ar1, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma parcial (PACF)\npacf(ar1, main = \"PACF\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-autorregresión-de-orden-p",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-de-autorregresión-de-orden-p",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.10 Procesos de autorregresión de orden p",
    "text": "3.10 Procesos de autorregresión de orden p\nUn proceso AR(\\(p\\)) se define mediante la ecuación:\n\\[\nY_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\cdots + \\phi_p Y_{t-p} + \\epsilon_t,\n\\]\ndonde:\n- \\(\\{\\epsilon_t\\}\\): Ruido blanco gaussiano (\\(\\epsilon_t \\sim N(0, \\sigma^2)\\)).\n- \\(c\\): Constante.\n- \\(\\phi_1, \\ldots, \\phi_p\\): Coeficientes autorregresivos.\n\n3.10.1 Estacionariedad de procesos AR(p)\nLa estacionariedad del proceso AR(\\(p\\)) requiere que todas las raíces de su ecuación característica:\n\\[\n1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p = 0,\n\\]\nse encuentren fuera del círculo unitario (\\(|z| &gt; 1\\)).\n\n\n3.10.2 Comportamiento de los correlogramas\n\nCorrelograma (ACF): Decae de manera lenta, reflejando la persistencia de los shocks pasados.\n\nCorrelograma parcial (PACF): Muestra un corte abrupto después del lag \\(p\\), es decir, \\(\\rho_{parcial}(j) = 0\\) para \\(j &gt; p\\).\n\nLos procesos AR(\\(p\\)) son útiles para modelar fenómenos con persistencia temporal, donde el valor actual depende de una combinación lineal de \\(p\\) observaciones anteriores.\nUn proceso autorregresivo de orden \\(p\\), AR(\\(p\\)), puede expresarse mediante el operador de traslación \\(B\\), donde \\(B^k Y_t = Y_{t-k}\\):\n\\[\n\\Phi(B) Y_t = c + \\varepsilon_t,\n\\]\ncon:\n\\[\n\\Phi(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p.\n\\]\n\n\n3.10.3 Condición de Invertibilidad\nSi todas las raíces \\(z\\) del polinomio característico:\n\\[\n\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p,\n\\]\nsatisfacen \\(|z| &gt; 1\\), entonces:\n1. \\(\\Phi(z) \\neq 0\\) para \\(|z| \\leq 1\\).\n2. El operador \\(\\Phi(B)\\) es invertible, permitiendo escribir:\n\\[\nY_t = c \\cdot \\Phi^{-1}(B) + \\Phi^{-1}(B) \\varepsilon_t.\n\\]\n\n\n3.10.4 Expansión en Serie de Potencias\nLa inversa de \\(\\Phi(B)\\) admite una representación en serie de potencias para \\(|z| \\leq 1\\):\n\\[\nY_t = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p} + \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j}.\n\\]\nLos coeficientes \\(\\psi_j\\) cumplen:\n\\[\n\\sum_{j=0}^\\infty |\\psi_j| &lt; \\infty,\n\\]\ngarantizando la convergencia de la serie y la estacionariedad del proceso.\n\n\n3.10.5 Estacionariedad de los procesos AR(p)\nUn proceso autorregresivo de orden \\(p\\), AR(\\(p\\)), puede representarse como un MA(\\(\\infty\\)) bajo condiciones específicas de sus coeficientes. Esta representación es clave para demostrar su estacionariedad.\n\n\n3.10.6 Condición sobre las raíces del polinomio característico\nEl polinomio característico asociado al proceso AR(\\(p\\)) es:\n\\[\n\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p.\n\\]\nPara garantizar estacionariedad, todas las raíces \\(z\\) de \\(\\Phi(z) = 0\\) deben ubicarse fuera del círculo unitario en el plano complejo:\n\\[\n|z| &gt; 1.\n\\]\n\n\n3.10.7 Relación con procesos MA(\\(\\infty\\))\nCuando se cumple la condición anterior:\n1. El operador \\(\\Phi(B)\\) asociado al proceso AR(\\(p\\)) es invertible.\n2. El proceso puede expresarse como:\n\\[\n   Y_t = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p} + \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j},\n   \\]\ndonde los coeficientes \\(\\psi_j\\) son absolutamente sumables:\n\\[\n   \\sum_{j=0}^\\infty |\\psi_j| &lt; \\infty.\n   \\]\n\n\n3.10.8 Estacionariedad\nLa representación MA(\\(\\infty\\)) asegura que el proceso AR(\\(p\\)) es estacionario en covarianza, ya que:\n- La media \\(\\mathbb{E}(Y_t)\\) es constante.\n- Las autocovarianzas \\(\\gamma_j\\) dependen únicamente del lag \\(j\\).\n- La convergencia de \\(\\sum \\psi_j^2\\) garantiza varianza finita.\nEsta conexión subraya que la estacionariedad del AR(\\(p\\)) está intrínsecamente ligada a la estructura de sus coeficientes y su polinomio característico.\n\nEjemplo\n\n\nCódigo\n### SEGUNDO EJEMPLO AR(4)\n\nset.seed(456)\n\n# Simular un AR(4): Y_t = 0.5*Y_{t-1} - 0.3*Y_{t-2} + 0.2*Y_{t-3}\n                        # + 0.1*Y_{t-4} + e_t\nar4 = arima.sim(n = 500, list(ar = c(0.5, -0.3, 0.2, 0.1)))\n\n# Graficar la serie\nts.plot(ar4, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 1)\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma (ACF)\nacf(ar4, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Correlograma parcial (PACF)\npacf(ar4, main = \"PACF\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-autorregresivos-media-móvil-armap-q",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#procesos-autorregresivos-media-móvil-armap-q",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.11 Procesos Autorregresivos-Media Móvil (ARMA(p, q))",
    "text": "3.11 Procesos Autorregresivos-Media Móvil (ARMA(p, q))\nUn proceso ARMA(p, q) combina componentes autorregresivos (AR) y de media móvil (MA) en una única ecuación:\n\\[\nY_t = c + \\phi_1 Y_{t-1} + \\cdots + \\phi_p Y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q},\n\\]\ndonde:\n- \\(\\{\\epsilon_t\\}\\): Ruido blanco con distribución \\(N(0, \\sigma^2)\\).\n- \\(c\\): Término constante que ajusta el nivel base de la serie.\n- \\(\\phi_1, \\ldots, \\phi_p\\): Coeficientes autorregresivos que vinculan \\(Y_t\\) con sus valores pasados.\n- \\(\\theta_1, \\ldots, \\theta_q\\): Coeficientes de media móvil que ponderan los errores pasados.\n\n3.11.1 Estacionariedad\nLa estacionariedad del proceso ARMA(p, q) depende exclusivamente de los coeficientes autorregresivos (\\(\\phi_j\\)). Para garantizarla, todas las raíces del polinomio característico:\n\\[\n\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p,\n\\]\ndeben ubicarse fuera del círculo unitario (\\(|z| &gt; 1\\)). Esta condición es idéntica a la requerida para los procesos AR(p).\n\n\n3.11.2 Comportamiento de los correlogramas\n\nFunción de Autocorrelación (ACF): Decae de manera exponencial o sinusoidal, sin cortes abruptos.\n\nFunción de Autocorrelación Parcial (PACF): También muestra un decaimiento gradual, similar al de la ACF.\n\nEstos patrones dificultan la identificación directa de los órdenes \\(p\\) y \\(q\\), ya que no presentan cortes claros. Por ello, se requieren técnicas complementarias, como el uso de correlaciones extendidas o criterios de información (AIC, BIC), para determinar los valores óptimos de \\(p\\) y \\(q\\).\nLos modelos ARMA(p, q) son herramientas versátiles para modelar series temporales con dependencias mixtas, capturando tanto la influencia de observaciones pasadas como la de shocks recientes. Su aplicación es fundamental en campos como econometría y forecasting.\n\n\n3.11.3 Representación MA(∞) de un proceso ARMA(p, q)\nUn proceso ARMA(p, q) puede expresarse mediante operadores de retardo \\(B\\) (\\(B^k Y_t = Y_{t-k}\\)) como:\n\\[\n\\Phi(B) Y_t = c + \\Theta(B) \\varepsilon_t,\n\\]\ndonde:\n- \\(\\Phi(B) = 1 - \\phi_1 B - \\cdots - \\phi_p B^p\\) (componente AR),\n- \\(\\Theta(B) = 1 + \\theta_1 B + \\cdots + \\theta_q B^q\\) (componente MA).\n\n\n3.11.4 Condición de estacionariedad\nLa estacionariedad del modelo depende de la componente autorregresiva (AR). Para garantizarla:\n- Todas las raíces \\(z\\) del polinomio \\(\\Phi(z) = 1 - \\phi_1 z - \\cdots - \\phi_p z^p\\) deben cumplir \\(|z| &gt; 1\\).\n- Esto asegura que \\(\\Phi(B)\\) sea invertible dentro del círculo unitario, permitiendo una representación MA(∞).\n\n\n3.11.5 Expansión en MA(∞)\nSi se satisface la condición anterior, el proceso ARMA(p, q) admite una representación de media móvil infinita:\n\\[\nY_t = \\frac{c}{1 - \\phi_1 - \\cdots - \\phi_p} + \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j},\n\\]\ndonde:\n- \\(\\mu = \\frac{c}{1 - \\phi_1 - \\cdots - \\phi_p}\\): Media constante del proceso.\n- \\(\\psi_j\\): Coeficientes obtenidos de la relación \\(\\frac{\\Theta(B)}{\\Phi(B)}\\), que cumplen \\(\\sum_{j=0}^\\infty |\\psi_j| &lt; \\infty\\).\n\n\n3.11.6 Estacionariedad\nLa convergencia absoluta de los coeficientes \\(\\psi_j\\) garantiza:\n- Varianza finita: \\(\\gamma_0 = \\sigma^2 \\sum_{j=0}^\\infty \\psi_j^2 &lt; \\infty\\).\n- Autocovarianzas dependientes solo del lag \\(j\\): \\(\\gamma_j = \\sigma^2 \\sum_{k=0}^\\infty \\psi_k \\psi_{k+j}\\).\nEsta estructura asegura que el proceso ARMA(p, q) sea estacionario en covarianza bajo las condiciones mencionadas.\n\n\n3.11.7 Características de los procesos ARMA(p, q)\n\nValor esperado\nLa media de un proceso ARMA(p, q) se obtiene mediante su representación como MA(∞) y está dada por:\n\\[\n\\mathbb{E}(Y_t) = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p}.\n\\]\nEsta expresión refleja el equilibrio entre el término constante \\(c\\) y la suma de los coeficientes autorregresivos (\\(\\phi_j\\)), garantizando una media constante en el tiempo.\n\n\nVarianza y autocovarianzas\nPara calcular la varianza (\\(\\gamma_0\\)) y las autocovarianzas (\\(\\gamma_j\\)), se analizan las desviaciones del proceso respecto a su media (\\(\\tilde{Y}_t = Y_t - \\mathbb{E}(Y_t)\\)).\n\nPara \\(j \\geq q + 1\\):\nLas autocovarianzas siguen una relación recursiva derivada de la componente AR:\n\\[\n\\gamma_j = \\phi_1 \\gamma_{j-1} + \\phi_2 \\gamma_{j-2} + \\cdots + \\phi_p \\gamma_{j-p}.\n\\]\nPara \\(j &lt; q + 1\\):\nEl cálculo se complica debido a la correlación entre los errores (\\(\\varepsilon_{t-j}\\)) y los valores pasados de \\(Y_t\\). Esto requiere resolver un sistema de ecuaciones que integra tanto los coeficientes AR (\\(\\phi_j\\)) como los MA (\\(\\theta_j\\)).\n\nLa varianza (\\(\\gamma_0\\)) y las autocovarianzas iniciales (\\(\\gamma_1, \\ldots, \\gamma_q\\)) no pueden derivarse directamente de la recursión y dependen de interacciones específicas entre los componentes AR y MA.\nNota: Las ecuaciones de Yule-Walker adaptadas para modelos ARMA permiten calcular estas cantidades, aunque su resolución exige métodos numéricos avanzados.\n\n\n\n3.11.8 Caso particular: Proceso ARMA(1, 1)\nUn proceso ARMA(1, 1) se define mediante la ecuación:\n\\[\nY_t = \\phi Y_{t-1} + \\varepsilon_t + \\theta \\varepsilon_{t-1},\n\\]\ndonde \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\) es ruido blanco.\n\nValor esperado\nDado que el modelo no incluye un término constante explícito, la media del proceso es:\n\\[\n\\mathbb{E}(Y_t) = 0.\n\\]\n\n\nRepresentación MA(∞)\nUtilizando el operador de retardo \\(B\\) (\\(B^k Y_t = Y_{t-k}\\)), el proceso puede reescribirse como:\n\\[\nY_t = (1 + \\phi B + \\phi^2 B^2 + \\cdots)(1 + \\theta B) \\varepsilon_t.\n\\]\nAl expandir esta expresión, obtenemos:\n\\[\nY_t = \\varepsilon_t + (\\phi + \\theta) \\sum_{i=1}^\\infty \\phi^{i-1} \\varepsilon_{t-i}.\n\\]\n\n\nVarianza\nLa varianza de \\(Y_t\\) se calcula considerando la contribución de todos los términos de la serie:\n\\[\n\\begin{aligned}\n\\text{Var}(Y_t) &= \\sigma^2 + (\\phi + \\theta)^2 \\sum_{i=1}^\\infty \\phi^{2(i-1)} \\sigma^2 \\\\\n&= \\sigma^2 \\left( 1 + (\\phi + \\theta)^2 \\cdot \\frac{1}{1 - \\phi^2} \\right) \\quad \\text{(serie geométrica)} \\\\\n&= \\sigma^2 \\left( 1 + \\frac{(\\phi + \\theta)^2}{1 - \\phi^2} \\right).\n\\end{aligned}\n\\]\nNota: La expresión anterior supone \\(|\\phi| &lt; 1\\) para garantizar la convergencia de la serie geométrica. Este resultado subraya cómo la varianza depende tanto de los coeficientes autorregresivos (\\(\\phi\\)) como de los de media móvil (\\(\\theta\\)).\n\n\n\n3.11.9 Cálculo de autocovarianzas para el proceso ARMA(1, 1)\nPara calcular la autocovarianza \\(\\gamma_k = \\mathbb{E}[Y_t Y_{t+k}]\\) en un modelo ARMA(1, 1), partimos de la representación MA(∞) del proceso:\n\\[\nY_t = \\varepsilon_t + (\\phi + \\theta) \\sum_{i=1}^\\infty \\phi^{i-1} \\varepsilon_{t-i}.\n\\]\nAl multiplicar \\(Y_t\\) y \\(Y_{t+k}\\), obtenemos:\n\\[\nY_t Y_{t+k} = \\left( \\varepsilon_t + (\\phi + \\theta) \\sum_{i=1}^\\infty \\phi^{i-1} \\varepsilon_{t-i} \\right) \\left( \\varepsilon_{t+k} + (\\phi + \\theta) \\sum_{j=1}^\\infty \\phi^{j-1} \\varepsilon_{t+k-j} \\right).\n\\]\n\nSimplificación de la esperanza\nDado que los errores \\(\\varepsilon_t\\) son independientes (\\(\\mathbb{E}[\\varepsilon_t \\varepsilon_s] = 0\\) para \\(t \\neq s\\)), solo sobreviven los términos donde los índices coinciden. Al tomar esperanza:\n\\[\n\\mathbb{E}[Y_t Y_{t+k}] = (\\phi + \\theta) \\phi^{k-1} \\sigma^2 + (\\phi + \\theta)^2 \\sigma^2 \\phi^k \\sum_{i=1}^\\infty \\phi^{2(i-1)}.\n\\]\n\nPrimer término:\nCorresponde a la contribución directa de los errores alineados en el lag \\(k\\):\n\\[\n(\\phi + \\theta) \\phi^{k-1} \\sigma^2.\n\\]\nSegundo término:\nInvolucra la serie geométrica generada por la interacción de los coeficientes:\n\\[\n(\\phi + \\theta)^2 \\sigma^2 \\phi^k \\cdot \\frac{1}{1 - \\phi^2} \\quad \\text{(suponiendo } |\\phi| &lt; 1 \\text{)}.\n\\]\n\n\n\nExpresión final de la autocovarianza\nCombinando ambos términos:\n\\[\n\\gamma_k = \\sigma^2 \\phi^{k-1} (\\phi + \\theta) \\left( 1 + \\frac{\\phi (\\phi + \\theta)}{1 - \\phi^2} \\right).\n\\]\nSimplificación adicional:\nPara \\(k = 0\\), esto se reduce a la varianza previamente calculada:\n\\[\n\\gamma_0 = \\sigma^2 \\left( 1 + \\frac{(\\phi + \\theta)^2}{1 - \\phi^2} \\right).\n\\]\nPara \\(k \\geq 1\\), la autocovarianza decae exponencialmente con el lag \\(k\\), característica típica de los procesos ARMA.\nNota: La condición \\(|\\phi| &lt; 1\\) es esencial para garantizar la convergencia de la serie geométrica y, por ende, la estacionariedad del proceso.\nLa k-ésima autocorrelación (\\(\\rho_k\\)) se obtiene al dividir la autocovarianza \\(\\gamma_k\\) entre la varianza \\(\\gamma_0\\):\n\\[\n\\rho_k = \\frac{\\phi^{k-1}(\\phi + \\theta) \\left( 1 + \\frac{\\phi(\\phi+\\theta)}{1+\\phi^2} \\right)}{1 + \\frac{(\\phi+\\theta)^2}{1+\\phi^2}}.\n\\]\nDe esta expresión, se deduce que las autocorrelaciones siguen una relación recursiva:\n\\[\n\\rho_k = \\phi \\rho_{k-1} \\quad \\text{para } k \\geq 1.\n\\]\nEsta relación refleja que el decaimiento de las autocorrelaciones está gobernado por el coeficiente autorregresivo \\(\\phi\\), independientemente del lag \\(k\\).\n\n\nEjemplo\n\n\nCódigo\n### ARMA(1,1) Y_t = 0.6 Y_{t-1} + e_t - 0.4e_{t-1}\n\narma11 = arima.sim(n = 500,\n          model = list(ar = 0.6, ma = -0.4))\n\n# Graficar series\nts.plot(arma11, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# ACF y PACF\nacf(arma11, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\npacf(arma11, main = \"PACF\")\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\nCódigo\n### ARMA (2,2) Y_t = 0.6 Y_{t-1} + 0.3 Y_{t-2} + e_t  \n                  # - 0.4e_{t-1} - 0.3 e_{t-2}\n\narma22 = arima.sim(n = 500,\n                   model = list(ar = c(0.6, 0.3),\n                                ma = c(-0.4, -0.3)))\n\n# Graficar series\nts.plot(arma22, main = \"Serie de tiempo\",\n        ylab = \"Y_t\", col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# ACF y PACF\nacf(arma22, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\npacf(arma22, main = \"PACF\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#motivación-ejemplo-de-diferenciación-para-eliminar-tendencias",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#motivación-ejemplo-de-diferenciación-para-eliminar-tendencias",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.12 Motivación: Ejemplo de diferenciación para eliminar tendencias",
    "text": "3.12 Motivación: Ejemplo de diferenciación para eliminar tendencias\n\n3.12.1 Serie con tendencia lineal\nConsidere una serie temporal \\(\\{Y_t\\}\\) definida como:\n\\[\nY_t = a + bt + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2).\n\\]\nEsta serie no es estacionaria debido a la tendencia lineal \\(bt\\).\nPrimera diferencia:\n\\[\n(1 - B)Y_t = Y_t - Y_{t-1} = b + (\\varepsilon_t - \\varepsilon_{t-1}).\n\\]\nLa tendencia lineal desaparece, y la serie diferenciada \\((1 - B)Y_t\\) es estacionaria.\n\n\n3.12.2 Serie con tendencia cuadrática\nSi la serie incluye una tendencia cuadrática:\n\\[\nY_t = a + bt + ct^2 + \\varepsilon_t, \\quad \\varepsilon_t \\sim N(0, \\sigma^2).\n\\]\nPrimera diferencia:\n\\[\n(1 - B)Y_t = Y_t - Y_{t-1} = b + c(2t - 1) + (\\varepsilon_t - \\varepsilon_{t-1}).\n\\]\nLa tendencia residual \\(c(2t - 1)\\) aún persiste.\nSegunda diferencia:\n\\[\n(1 - B)^2 Y_t = Y_t - 2Y_{t-1} + Y_{t-2}.\n\\]\nAl aplicar la segunda diferencia, se elimina la tendencia cuadrática, resultando en una serie estacionaria \\((1 - B)^2 Y_t\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#proceso-autorregresivo-integrado-de-medias-móviles-arima",
    "href": "Caps/03-Modelos lineales para series de tiempo estacionarias.html#proceso-autorregresivo-integrado-de-medias-móviles-arima",
    "title": "3  Modelos lineales para series de tiempo estacionarias",
    "section": "3.13 Proceso autorregresivo integrado de medias móviles (ARIMA)",
    "text": "3.13 Proceso autorregresivo integrado de medias móviles (ARIMA)\nUn proceso ARIMA(p, d, q) se emplea para modelar series temporales no estacionarias que pueden transformarse en estacionarias mediante diferenciación. Una serie \\(\\{Y_t\\}\\) se clasifica como no estacionaria homogénea si, aunque no es estacionaria en su forma original, al aplicar diferencias de orden \\(d\\):\n\\[\nw_t = (1 - B)^d Y_t,\n\\]\nse obtiene una serie \\(\\{w_t\\}\\) estacionaria. Aquí, \\(B\\) denota el operador de retardo (\\(B Y_t = Y_{t-1}\\)).\nEl modelo ARIMA(p, d, q) se define mediante la ecuación:\n\\[\n\\Phi(B)(1 - B)^d Y_t = \\delta + \\Theta(B) \\varepsilon_t,\n\\]\ndonde:\n- \\(\\Phi(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p\\) representa el componente autorregresivo de orden \\(p\\).\n- \\(\\Theta(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\cdots + \\theta_q B^q\\) corresponde al componente de medias móviles de orden \\(q\\).\n- \\((1 - B)^d\\) es el operador de diferenciación de orden \\(d\\), que elimina tendencias o patrones no estacionarios.\n- \\(\\delta\\) es un término constante ajustado tras la diferenciación.\n- \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\) es ruido blanco.\nEl proceso de modelado implica tres etapas principales: diferenciar la serie \\(d\\) veces para lograr estacionariedad, identificar los órdenes \\(p\\) y \\(q\\) para el modelo ARMA aplicado a la serie diferenciada, y finalmente estimar y validar los parámetros del modelo.\nEste enfoque integra la capacidad de los modelos ARMA para capturar dependencias en series estacionarias, combinándola con la diferenciación para manejar no estacionariedades en los datos originales.\n\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales para series de tiempo estacionarias</span>"
    ]
  },
  {
    "objectID": "Caps/04-Ajuste del modelo a una serie de tiempo.html",
    "href": "Caps/04-Ajuste del modelo a una serie de tiempo.html",
    "title": "4  Ajuste del modelo a una serie de tiempo",
    "section": "",
    "text": "4.1 Identificación del Modelo\nSe utiliza un procedimiento iterativo de tres pasos para construir un modelo ARIMA.\nLos esfuerzos de identificación del modelo deben comenzar con intentos preliminares de comprender el tipo de proceso del que provienen los datos y cómo se recolectan. Las características percibidas del proceso y la frecuencia de muestreo a menudo proporcionan información valiosa en esta etapa preliminar de la identificación del modelo. En los entornos actuales, ricos en datos, a menudo se espera que los profesionales dispongan de “suficientes” datos para poder generar modelos confiables.\nAntes de llevar a cabo esfuerzos rigurosos de construcción de modelos estadísticos, también se recomienda encarecidamente el uso de gráficos “creativos” de los datos, como el simple gráfico de series temporales y los diagramas de dispersión de los datos de series temporales \\(Y_t\\) frente a \\(Y_{t-1}, Y_{t-2},...\\). Los gráficos simples de series de tiempo deben usarse como herramienta de evaluación preliminar para la estacionariedad. La inspección visual de estos gráficos debería confirmarse más adelante como se describió anteriormente en este capítulo. Si se sospecha de no estacionariedad, también debe considerarse el gráfico de series temporales de la primera (o \\(d\\)-ésima) diferencia. También se puede realizar la prueba de Dickey y Fuller para asegurarse de que efectivamente se necesita la diferenciación. Una vez que se puede asumir la estacionariedad, se deben obtener la ACF y PACF de la serie temporal original (o de su \\(d\\)-ésima diferencia si es necesario).\nLa identificación de modelos ARMA requeriría más cuidado, ya que tanto la ACF como la PACF mostrarán una decaimiento exponencial y/o comportamiento sinusoidal amortiguado. La identificación del modelo ARIMA apropiado requiere habilidades adquiridas por experiencia.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ajuste del modelo a una serie de tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Ajuste del modelo a una serie de tiempo.html#estimación-de-parámetros",
    "href": "Caps/04-Ajuste del modelo a una serie de tiempo.html#estimación-de-parámetros",
    "title": "4  Ajuste del modelo a una serie de tiempo",
    "section": "4.2 Estimación de parámetros",
    "text": "4.2 Estimación de parámetros\nExisten varios métodos, como los métodos de momentos, máxima verosimilitud y mínimos cuadrados, que pueden emplearse para estimar los parámetros en el modelo identificado tentativamente. Sin embargo, a diferencia de los modelos de regresión, la mayoría de los modelos ARIMA son modelos no lineales y requieren el uso de un procedimiento de ajuste no lineal. No obstante, esto generalmente se realiza de manera automática mediante paquetes de software sofisticados como Minitab, JMP y SAS. En algunos paquetes de software, el usuario puede tener la opción de elegir el método de estimación y, en consecuencia, seleccionar el método más apropiado según las especificaciones del problema.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ajuste del modelo a una serie de tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Ajuste del modelo a una serie de tiempo.html#verificación-de-diagnóstico",
    "href": "Caps/04-Ajuste del modelo a una serie de tiempo.html#verificación-de-diagnóstico",
    "title": "4  Ajuste del modelo a una serie de tiempo",
    "section": "4.3 Verificación de Diagnóstico",
    "text": "4.3 Verificación de Diagnóstico\nDespués de ajustar un modelo tentativo a los datos, se debe de examinar su adecuación y, si es necesario, sugerir posibles mejoras. Esto se hace mediante el análisis de residuos. Los residuos para un proceso ARMA (\\(p,q\\)) pueden obtenerse mediante: \\[ \\hat{\\varepsilon}_t=Y_t-\\left(\\hat{\\delta}+\\sum_{i=1}^p\\hat{\\phi}_iY_{t-i}-\\sum_{i=1}^q\\hat{\\theta}_i\\hat{\\varepsilon}_{t-i}\\right)\\] Si el modelo especificado es adecuado y, por lo tanto, se han identificado los órdenes apropiados \\(p\\) y \\(q\\) debería transformar las observaciones en un proceso de ruido blanco. Así, los residuos en la ecuación anterior deberían comportarse como ruido blanco.\nDenotemos la función de autocorrelación muestral de los residuos como \\(\\{r_e(k)\\}\\). Si el modelo es apropiado, entonces la función de autocorrelación muestral de los residuos no debería presentar estructura alguna para identificar. Es decir, la autocorrelación no debería diferir significativamente de cero para todos los rezagos mayores que uno.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ajuste del modelo a una serie de tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Ajuste del modelo a una serie de tiempo.html#pronósticos-de-un-proceso-arima",
    "href": "Caps/04-Ajuste del modelo a una serie de tiempo.html#pronósticos-de-un-proceso-arima",
    "title": "4  Ajuste del modelo a una serie de tiempo",
    "section": "4.4 Pronósticos de un proceso ARIMA",
    "text": "4.4 Pronósticos de un proceso ARIMA\nUna vez que se ha ajustado un modelo de series de tiempo apropiado, puede utilizarse para generar pronósticos de observaciones futuras. Si denotamos el tiempo actual como \\(T\\), el pronóstico para \\(Y_{T+\\tau}\\) se llama pronóstico a \\(\\tau-\\)pasos pasos adelante y se denota como \\(\\hat{Y}_{T+\\tau}(T)\\) El criterio estándar utilizado para obtener el mejor pronóstico es el error cuadrático medio, para el cual se minimiza el valor esperado del cuadrado de los errores de pronóstico: \\[ \\mathbb{E}[(Y_{T+\\tau}-\\hat{Y}_{T+\\tau}(T))^2]=\\mathbb{E}[e_T(\\tau)^2].\\] Se puede demostrar que el mejor pronóstico en el sentido de error cuadrático medio es la esperanza condicional de \\(Y_{T+\\tau}\\) dado \\(Y_{T},Y-{T-1},...,\\) es decir: \\[ \\hat{Y}_{T+\\tau}=\\mathbb{E}[Y_{T+\\tau}|Y_T,Y_{T-1},...].\\] Consideremos, por ejemplo, un proceso ARIMA(\\(p,d,q\\)) en el tiempo \\(T+\\tau\\). Consideremos además su representación MA infinita: \\[ Y_{T+\\tau}=\\mu+\\sum_{i=1}^\\infty \\psi_i\\varepsilon_{T+\\tau-i}. \\] Podemos particionar la ecuación anterior como \\[Y_{T+\\tau}=\\mu+\\sum_{i=1}^{\\tau-1} \\psi_i\\varepsilon_{T+\\tau-i}+\\sum_{i=\\tau}^\\infty \\psi_i\\varepsilon_{T+\\tau-i}.\\]\nEn esta partición, podemos ver que el componente \\(\\sum_{i=1}^{\\tau-1} \\psi_i\\varepsilon_{T+\\tau-i}\\) involucra errores futuros, mientras que el componente \\(\\sum_{i=\\tau}^\\infty \\psi_i\\varepsilon_{T+\\tau-i}\\) involucra errores presentes y pasados. A partir de la relación entre las observaciones actuales y pasadas y los choques aleatorios correspondientes, así como el hecho de que se asume que los choques aleatorios tienen media cero y son independientes, se puede demostrar que el mejor pronóstico en el sentido de error cuadrático medio es: \\[ \\hat{Y}_{T+\\tau}=\\mathbb{E}[Y_{T+\\tau}|Y_T,Y_{T-1},...]=\\mu+ \\sum_{i=\\tau}^\\infty \\psi_i\\varepsilon_{T+\\tau-i},\\] dado que: \\[ \\mathbb{E}[\\varepsilon_{T+\\tau-i}|Y_T,Y_{T-1},...]=\\begin{cases} 0 & \\text{si } i &lt; \\tau \\\\ \\varepsilon_{T+\\tau-i} & \\text{si } i\\geq\\tau \\end{cases}. \\] Posteriormente, el error de pronóstico se calcula a partir de: \\[e_T(\\tau)=Y_{T\\tau}-\\hat{Y}_{T+\\tau}(T)=\\sum_{i=0}^{\\tau-1}\\psi_i\\varepsilon_{T+\\tau-i}. \\]\nDado que el error de pronóstico es una combinación lineal de choques aleatorios, tenemos: \\[\\mathbb{E}=(e_T(\\tau))=0 \\\\ Var(e_T(\\tau))=\\sum_{i=0}^{\\tau-1}\\psi^2_iVar(\\varepsilon_{T+\\tau-i})=\\sigma^2\\sum_{i=0}^{\\tau-1}\\psi^2=\\sigma^2(\\tau), \\ \\ \\ \\tau=1,2,... .\\] Debe observarse que la varianza del error de pronóstico aumenta con el tiempo de adelanto \\(\\tau\\). Esto tiene sentido intuitivamente, ya que deberíamos esperar más incertidumbre en nuestros pronósticos cuanto más nos alejamos en el futuro. Además, si los choques aleatorios están normalmente distribuidos \\(N(0,\\sigma^2)\\) entonces los errores de pronóstico también estarán distribuidos normalmente como \\(N(0,\\sigma^2(\\tau))\\). Podemos entonces obtener los intervalos de predicción al \\(100(1-\\alpha)\\%\\) para las observaciones futuras a partir de: \\[ \\mathbb{P}[\\hat{Y}_{T+\\tau}(T)-z_{\\alpha/2}\\sigma(\\tau)&lt;Y_{T+\\tau}&lt;\\hat{Y}_{T+\\tau}(T)+z_{\\alpha/2}\\sigma(\\tau)]=1-\\alpha,\\] donde \\(z_{\\alpha/2}\\) es el percentil superior \\(\\alpha/2\\) de la distribución normal estándar. Por lo tanto el intervalo de predicción al \\(100(1-\\alpha)\\%\\) para \\(Y_{T+\\tau}\\) es \\[ \\hat{Y}_{T+\\tau}(T)\\pm z_{\\alpha/2}\\sigma(\\tau).\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ajuste del modelo a una serie de tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Ajuste del modelo a una serie de tiempo.html#ejemplos-de-ajuste-de-datos",
    "href": "Caps/04-Ajuste del modelo a una serie de tiempo.html#ejemplos-de-ajuste-de-datos",
    "title": "4  Ajuste del modelo a una serie de tiempo",
    "section": "4.5 Ejemplos de ajuste de datos",
    "text": "4.5 Ejemplos de ajuste de datos\n\n4.5.1 Ventas de productos farmacéuticos\nEl conjunto de datos que estudiaremos en esta sección son ventas de productos farmacéuticos en Estados Unidos.\nAnalizamos la serie de tiempo, así como sus ACF, PACF y su primer diferencia.\nEn base a lo anterior escogeremos un modelo ARIMA a la serie de tiempo.\n\n\nCódigo\n                              ### VENTAS DE FARMACEÚTICOS ###\n\n### Librerías necesarias \nlibrary(nortest) # Anderson-Darling\nlibrary(forecast) # Librería necesaria para la función Arima\nlibrary(tseries) # Prueba Dickey-Fuller para estacionariedad\n\nventas = c(\n  10618.1, 10537.9, 10209.3, 10553.0,  9934.9, 10534.5, 10196.5, 10511.8, 10089.6, 10371.2,\n  10239.4, 10472.4, 10827.2, 10640.8, 10517.8, 10154.2,  9969.2, 10260.4, 10737.0, 10430.0,\n  10689.0, 10430.4, 10002.4, 10135.7, 10096.2, 10288.7, 10289.1, 10589.9, 10551.9, 10208.3,\n  10334.5, 10480.1, 10387.6, 10202.6, 10219.3, 10382.7, 10820.5, 10358.7, 10494.6, 10497.6,\n  10431.5, 10447.8, 10684.4, 10176.5, 10616.0, 10627.7, 10684.0, 10246.7, 10265.0, 10090.4,\n  9881.1, 10449.7, 10276.3, 10175.2, 10212.5, 10395.5, 10545.9, 10635.7, 10265.2, 10551.6,\n  10538.2, 10286.2, 10171.3, 10393.1, 10162.3, 10164.5, 10327.0, 10365.1, 10755.9, 10463.6,\n  10080.5, 10479.6,  9980.9, 10039.2, 10246.1, 10368.0, 10446.3, 10535.3, 10786.9,  9975.8,\n  10160.9, 10422.1, 10757.2, 10463.8, 10307.0, 10134.7, 10207.7, 10488.0, 10262.3, 10785.9,\n  10375.4, 10123.4, 10462.7, 10205.5, 10522.7, 10253.2, 10428.7, 10615.8, 10417.3, 10445.4,\n  10690.6, 10271.8, 10524.8,  9815.0, 10398.5, 10553.1, 10655.8, 10199.1, 10416.6, 10391.3,\n  10210.1, 10352.5, 10423.8, 10519.3, 10596.7, 10650.0, 10741.6, 10246.0, 10354.4, 10155.4\n)\n\n# Gráfica de la serie de tiempo\nventas_ts = ts(ventas, start = 1, frequency = 1) # Guardamos los datos como una serie de tiempo\nplot(ventas_ts, main = \"Ventas\",\n     xlab = \"Semana\", ylab = \"Ventas (miles)\",\n     col = \"#F4B4C9\", lwd = 2) \n\n\n\n\n\n\n\n\n\nCódigo\nadf.test(ventas_ts) # Prueba de Dickey-Fuller para estacionariedad.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ventas_ts\nDickey-Fuller = -6.2859, Lag order = 4, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCódigo\n# Revisamos ACF y PACF\nacf(ventas_ts, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\npacf(ventas_ts, main = \"PACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Ajustamos un modelo ARIMA automáticamente.\n\nmodelo = Arima(ventas_ts, order = c(3,0,3)) # Seleccionamos Arima\n#modelo = auto.arima(ventas_ts)\nsummary(modelo)\n\n\nSeries: ventas_ts \nARIMA(3,0,3) with non-zero mean \n\nCoefficients:\n         ar1      ar2     ar3      ma1     ma2      ma3        mean\n      1.7218  -1.4351  0.5872  -1.6731  1.2966  -0.6235  10373.7762\ns.e.  0.4589   0.5842  0.2299   0.4487  0.5242   0.1921      3.5664\n\nsigma^2 = 43441:  log likelihood = -808.7\nAIC=1633.4   AICc=1634.7   BIC=1655.7\n\nTraining set error measures:\n                   ME     RMSE      MAE         MPE     MAPE      MASE\nTraining set 1.117581 202.2538 169.5169 -0.02774167 1.634562 0.7174565\n                    ACF1\nTraining set -0.02722326\n\n\nCódigo\nresiduales = as.numeric(modelo$residuals) # Guardamos residuales.\najustados = as.numeric(modelo$fitted) # Guardamos valores ajustados.\n\n# Gráfica de ajustados vs residuales\nplot(ajustados, residuales, \n     xlab = \"Valores ajustados\", \n     ylab = \"Residuales\", \n     main = \"Residuales vs Ajustados\",\n     pch = 16,  # punto sólido\n     col = \"#F4B4C9\")\nabline(h = 0, col = \"#F06292\", lty = 2)\n\n\n\n\n\n\n\n\n\nCódigo\ncheckresiduals(modelo)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(3,0,3) with non-zero mean\nQ* = 0.87451, df = 4, p-value = 0.9282\n\nModel df: 6.   Total lags used: 10\n\n\nCódigo\nad.test(residuales) # Prueba de normalidad para los residuales\n\n\n\n    Anderson-Darling normality test\n\ndata:  residuales\nA = 0.42528, p-value = 0.3115\n\n\nCódigo\nqqnorm(residuales)\nqqline(residuales, col = \"red\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Gráfica de la serie de tiempo\nplot(ventas_ts, main = \"Ventas\",\n     xlab = \"Semana\", ylab = \"Ventas (miles)\",\n     col = \"#F4B4C9\", lwd = 2) \n#points(ventas_ts, pch = 16, col = \"#F4B4C9\") # Graficamos los datos con circulos\n\najustados_ts = ts(ajustados)\n\n# Encimamos los valores ajustados.\nlines(ajustados_ts, col = \"#F06292\", lwd = 2)\n#points(ajustados, col = \"#CE93D8\", lwd = 2, pch = 16)\n\n# Añadir leyenda\nlegend(\"topleft\", legend = c(\"Original\", \"Ajustado\"),\n       col = c(\"#F4B4C9\", \"#F06292\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Producción de Queso y Gorgonzola en EUA\nAnalizamos un conjunto de datos referente al a producción de queso azul y gorgonzola en Estados Unidos. En la gráfica de la serie de tiempo podremos observar que es evidentemente no estacionaria por lo que un análisis de sus diferencias es crucial.\nEstudiaremos sus diferencias rigurosamente buscando volver estacionaria la serie, para finalmente ajustar un modelo ARIMA en base a la información proporcionada por sus ACF y PACF.\n\n\nCódigo\n                                  ### PRODUCCIÓN DE QUESO ###\n\n### Librerías necesarias \nlibrary(nortest) # Anderson-Darling\nlibrary(forecast) # Librería necesaria para la función Arima\nlibrary(tseries) # Prueba Dickey-Fuller para estacionariedad\n\nqueso = c(\n  7657, 5451, 10883, 9554, 9519, 10047, 10663, 10864, 11447, 12710, \n  15169, 16205, 14507, 15400, 16800, 19000, 20198, 18573, 19375, 21032, \n  23250, 25219, 28549, 29759,\n  28262, 28506, 33885, 34776, 35347, 34628, 33043, 30214, 31013, 31496, \n  34115, 33433, 34198, 35863, 37789, 34561, 36434, 34371, 33307, 33295, \n  36514, 36593, 38311, 42773\n)\n\n# Gráfica de la serie de tiempo\nqueso_ts = ts(queso, start = 1950, frequency = 1) # Guardamos los datos como una serie de tiempo\nplot(queso_ts, main = \"Producción de queso\",\n     xlab = \"Año\", ylab = \"Producción (miles de libras)\",\n     col = \"#F4B4C9\", lwd = 2) \n\n\n\n\n\n\n\n\n\nCódigo\n#points(queso_ts, pch = 16, col = \"#CE93D8\") # Graficamos los datos con circulos\n\n########## Revisamos la primer diferencia\n\n# Gráfica de la serie de tiempo\nqueso_diff = ts(diff(queso_ts))\nplot(queso_diff, main = \"Primera diferencia\",\n     xlab = \"Diferencia\", ylab = \"Producción (miles de libras)\",\n     col = \"#F4B4C9\", lwd = 2) \n\n\n\n\n\n\n\n\n\nCódigo\n#points(queso_diff, pch = 16, col = \"#CE93D8\") # Graficamos los datos con circulos\n\n\nadf.test(queso_diff) # Prueba de Dickey-Fuller para estacionariedad.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  queso_diff\nDickey-Fuller = -2.8641, Lag order = 3, p-value = 0.2297\nalternative hypothesis: stationary\n\n\nCódigo\n########## Revisamos la segunda diferencia\n\n# Gráfica de la serie de tiempo\nqueso_diff2 = ts(diff(queso_diff))\nplot(queso_diff2, main = \"Segunda diferencia\",\n     xlab = \"Diferencia\", ylab = \"Producción (miles de libras)\",\n     col = \"#F4B4C9\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n#points(queso_diff2, pch = 16, col = \"#CE93D8\") # Graficamos los datos con circulos\n\n\nadf.test(queso_diff2) # Prueba de Dickey-Fuller para estacionariedad.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  queso_diff2\nDickey-Fuller = -4.2795, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCódigo\n# Revisamos ACF y PACF de la segunda diferencia\nacf(queso_diff2, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\npacf(queso_diff2, main = \"PACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Los correlogramas indican un ARIMA(1,2,1), posiblemente.\n\n# Ajustamos un modelo ARIMA\n\n\nmodelo = Arima(queso_ts, order = c(1,2,1)) # Seleccionamos el modelo arima\n#modelo = auto.arima(queso_ts) #Se selecciona el \"mejor\" arima algorítmicamente.\nsummary(modelo)\n\n\nSeries: queso_ts \nARIMA(1,2,1) \n\nCoefficients:\n          ar1     ma1\n      -0.0542  -1.000\ns.e.   0.1577   0.065\n\nsigma^2 = 3848217:  log likelihood = -414.98\nAIC=835.96   AICc=836.53   BIC=841.44\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 80.04719 1878.173 1369.098 0.5073474 6.078675 0.8437926 0.03793247\n\n\nCódigo\nresiduales = as.numeric(modelo$residuals) # Guardamos residuales.\najustados = as.numeric(modelo$fitted) # Guardamos valores ajustados.\n\n# Gráfica de ajustados vs residuales\nplot(ajustados, residuales, \n     xlab = \"Valores ajustados\", \n     ylab = \"Residuales\", \n     main = \"Residuales vs Ajustados\",\n     pch = 16,  # punto sólido\n     col = \"#F4B4C9\")\n\nabline(h = 0, col = \"#F06292\", lty = 2)\n\n\n\n\n\n\n\n\n\nCódigo\ncheckresiduals(modelo)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,2,1)\nQ* = 5.0046, df = 8, p-value = 0.7571\n\nModel df: 2.   Total lags used: 10\n\n\nCódigo\nad.test(residuales) # Prueba de normalidad para los residuales\n\n\n\n    Anderson-Darling normality test\n\ndata:  residuales\nA = 0.61519, p-value = 0.1034\n\n\nCódigo\nqqnorm(residuales)\nqqline(residuales, col = \"red\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Gráfica de la serie de tiempo\nplot(queso_ts, main = \"Producción de queso\",\n     xlab = \"Año\", ylab = \"Producción (miles de libras)\",\n     col = \"#F4B4C9\", lwd = 2) \n#points(queso, pch = 16, col = \"#F4B4C9\") # Graficamos los datos con circulos\n\najustados_ts = ts(ajustados, start = 1950, frequency = 1)\n\n# Encimamos los valores ajustados.\nlines(ajustados_ts, col = \"#F06292\", lwd = 2)\n#points(ajustados, col = \"#CE93D8\", lwd = 2, pch = 16)\n\n# Añadir leyenda\nlegend(\"topleft\", legend = c(\"Original\", \"Ajustado\"),\n       col = c(\"#F4B4C9\", \"#F06292\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ajuste del modelo a una serie de tiempo</span>"
    ]
  },
  {
    "objectID": "Caps/05-Caso de estudio.html",
    "href": "Caps/05-Caso de estudio.html",
    "title": "5  Casos de Estudios",
    "section": "",
    "text": "Ahora estudiaremos la cantidad de azufre minada mensualmente en Guanajuato desde el año 2000 al 2018.\nEstos datos son obtenidos del Banco de Indicadores del Inegi.\nBuscaremos aplicar un modelo ARIMA a dicha serie temporal, ajustar los datos y finalmente mostrar las predicciones para los datos del año 2019.\n\n\nCódigo\n### EJEMPLO DE AZUFRE ###\n\n### Librerías necesarias \nlibrary(nortest) # Anderson-Darling\nlibrary(forecast) # Librería necesaria para la función Arima\nlibrary(tseries) # Prueba Dickey-Fuller para estacionariedad\n\nazufre = c(2108, 2407, 2697, 2550, 2635, 2730, 3069, 2480, 1950, 2511, 2370,\n           2108, 2356, 2072, 1860, 2190, 2511, 2670, 2945, 2697, 2940, 2325,\n           2310, 2263, 2976, 2744, 2418, 1800, 930, 2130, 2667, 2001, 2106,\n           2350, 2091, 2651, 2171, 2614, 2692, 2285, 2130, 1979, 2063, 2222,\n           2224, 2320, 2572, 2206, 2698, 2719, 2747, 2680, 2575, 2827, 3004,\n           3119, 3426, 3645, 3512, 3247, 3220, 3062, 3566, 2920, 3018, 1848,\n           2627, 2821, 2445, 2676, 1937, 2785, 3281, 2798, 2927, 3461, 3580,\n           3581, 3454, 3240, 3110, 3308, 2934, 3266, 2822, 2712, 2472, 3444,\n           3415, 3475, 2511, 3032, 2503, 2155, 2131, 2658, 2916, 3168, 3632,\n           3612, 3232, 3229, 3624, 3990, 2723, 2401, 3514, 3898, 3533, 3279,\n           3876, 3754, 3706, 2298, 3059, 3371, 2780, 2886, 3304, 3715, 3084,\n           3110, 3711, 3329, 2863, 3411, 2579, 3270, 3205, 2401, 2318, 2621,\n           2556, 2371, 2985, 2999, 2454, 3205, 3107, 2292, 1831, 1339, 1882,\n           2632, 2051, 2272, 2893, 3021, 3007, 1760, 2396, 3095, 2616, 2084,\n           2921, 3089, 2841, 2755, 3298, 3026, 2928, 3156, 2597, 2149, 2339,\n           2513, 2124, 2617, 2746, 2058, 2418, 1690, 2305, 2191, 2215, 2695,\n           1632, 1196, 1375, 1175, 903, 910, 1276, 1232, 1138, 888, 1322,\n           1540, 1743, 1386, 731, 1350, 1832, 1317, 1982, 2477, 2220, 1772,\n           1983, 2356, 2132, 2302, 2067, 2176, 2158, 2054, 2196, 1925, 1964,\n           1273, 1364, 1229, 1211, 1119, 1067, 1213, 1058, 839, 1653, 1337,\n           1349, 1347, 1311, 1302, 1510, 535, 368, 864)\n\naz = ts(azufre, start = c(2000, 1), frequency = 12)\n\n# Gráfica de la serie de tiempo\nplot(az, main = \"Minería mensual de azufre en Guanajuato\",\n     xlab = \"Mes\", ylab = \"Toneladas\",\n     col = \"#F4B4C9\", lwd = 2) \n\n\n\n\n\n\n\n\n\nCódigo\n#points(az, pch = 16, col = \"#CE93D8\") # Graficamos los datos con circulos\n\nadf.test(az) # Prueba de Dickey-Fuller para estacionariedad.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  az\nDickey-Fuller = -2.2567, Lag order = 6, p-value = 0.4681\nalternative hypothesis: stationary\n\n\nCódigo\n########## Revisamos la primer diferencia\n\naz_diff = ts(diff(az))\n\n# Gráfica de la serie de tiempo\nplot(az_diff, main = \"Primera diferencia\",\n     xlab = \"Diferencia\", ylab = \"Toneladas\",\n     col = \"#F4B4C9\", lwd = 2) \n\n\n\n\n\n\n\n\n\nCódigo\n#points(az_diff, pch = 16, col = \"#CE93D8\") # Graficamos los datos con circulos\n\nadf.test(az_diff) # Prueba de Dickey-Fuller para estacionariedad.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  az_diff\nDickey-Fuller = -7.9615, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCódigo\n# Revisamos ACF y PACF de la segunda diferencia\nacf(az_diff, main = \"ACF\")\n\n\n\n\n\n\n\n\n\nCódigo\npacf(az_diff, main = \"PACF\")\n\n\n\n\n\n\n\n\n\nCódigo\n# Ajustamos un modelo ARIMA\n\nmodelo = Arima(az, order = c(2,1,2))\nmodelo = auto.arima(az)\nsummary(modelo)\n\n\nSeries: az \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3434  -0.2917\ns.e.   0.0649   0.0684\n\nsigma^2 = 169125:  log likelihood = -1687.67\nAIC=3381.35   AICc=3381.46   BIC=3391.62\n\nTraining set error measures:\n                    ME    RMSE      MAE       MPE     MAPE      MASE\nTraining set -15.69207 408.533 324.3837 -4.777137 16.18093 0.5387357\n                    ACF1\nTraining set -0.01066291\n\n\nCódigo\nresiduales = as.numeric(modelo$residuals) # Guardamos residuales.\najustados = as.numeric(modelo$fitted) # Guardamos valores ajustados.\ncheckresiduals(modelo)\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)\nQ* = 26.786, df = 22, p-value = 0.2195\n\nModel df: 2.   Total lags used: 24\n\n\nCódigo\n# Gráfica de ajustados vs residuales\nplot(ajustados, residuales, \n     xlab = \"Valores ajustados\", \n     ylab = \"Residuales\", \n     main = \"Residuales vs Ajustados\",\n     pch = 16,  # punto sólido\n     col = \"#F4B4C9\")\nabline(h = 0, col = \"#F06292\", lty = 2)\n\n\n\n\n\n\n\n\n\nCódigo\n# Prueba de Ljung-Box para analizar la autocorrelación de los residuales\n#Box.test(residuales, lag = 40, type = \"Ljung-Box\")\n\n\nad.test(residuales) # Prueba de normalidad para los residuales\n\n\n\n    Anderson-Darling normality test\n\ndata:  residuales\nA = 0.99606, p-value = 0.01236\n\n\nCódigo\nqqnorm(residuales)\nqqline(residuales, col = \"red\")\n\n\n\n\n\n\n\n\n\nCódigo\nhist(residuales)\n\n\n\n\n\n\n\n\n\nCódigo\n# Gráfica de la serie de tiempo\nplot(az, main = \"Minería mensual de azufre en Guanajuato\",\n     xlab = \"Mes\", ylab = \"Toneladas\",\n     col = \"#F4B4C9\", lwd = 2) \n#points(az, pch = 16, col = \"#F4B4C9\") # Graficamos los datos con circulos\n\najustados_ts = ts(ajustados, , start = c(2000, 1), frequency = 12)\n\n# Encimamos los valores ajustados.\nlines(ajustados_ts, col = \"#F06292\", lwd = 2)\n#points(ajustados, col = \"#CE93D8\", lwd = 2, pch = 16)\n\n# Añadir leyenda\nlegend(\"topleft\", legend = c(\"Original\", \"Ajustado\"),\n       col = c(\"#F4B4C9\", \"#F06292\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nCódigo\npredic = forecast(modelo,12)\nplot(predic, col = \"#F06292\", lwd = 2)\nreales = c(0, 493, 1246, 1428, 1711, 1585, 1587, 1297, 313, 44, 0,133)\nreales_ts = ts(reales, start = c(2019,1), frequency = 12)\nlines(reales_ts, col = \"orange\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbraham, Bovas, y Johannes Ledolter. 1983. Statistical Methods for Forecasting. 1.ª ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., y Andrew V. Metcalfe. 2009. Introductory Time Series with R. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. Baltimore: Springer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1.ª ed. New York: Wiley.\n\n\nHamilton, J. D. 2020. Times series Analysis. 1.ª ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, y Murat Kulahci. 2008. Introduction to Time Series Analysis and Forecasting. 1.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Casos de Estudios</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Abraham, Bovas, and Johannes Ledolter. 1983. Statistical Methods for\nForecasting. 1st ed. New Jersey: Wiley.\n\n\nCowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory\nTime Series with r. 1st ed. Wiley Series in Probability and\nMathematical Statistics. Applied Probability and Statistics. Baltimore:\nSpringer.\n\n\nEnders, Walter. 2015. Applied Econometric Time Series. 1st ed.\nNew York: Wiley.\n\n\nHamilton, J. D. 2020. Times Series Analysis. 1st ed. Wiley.\n\n\nMontgomery, Douglas C., Cheryl L. Jennings, and Murat Kulahci. 2008.\nIntroduction to Time Series Analysis and Forecasting. 1st ed.\nWiley Series in Probability and Mathematical Statistics. Applied\nProbability and Statistics. New York: Wiley.",
    "crumbs": [
      "Referencias"
    ]
  }
]